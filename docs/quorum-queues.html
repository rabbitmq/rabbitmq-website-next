<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta content="73d8ba46-8c12-43f6-8c22-24aa21b8d93d" name="onetrust-data-domain" /><meta content="https://tags.tiqcdn.com/utag/vmware/microsites-privacy/prod/utag.js" name="microsites-utag" /><script src="https://d1fto35gcfffzn.cloudfront.net/assets/jquery-1.11.2.min.js"></script><script src="//www.vmware.com/files/templates/inc/utag_data.js"></script><script src="//tags.tiqcdn.com/utag/vmware/microsites-privacy/prod/utag.sync.js"></script><script>function OptanonWrapper() { { window.dataLayer.push({ event: 'OneTrustGroupsUpdated' }); } }</script><script src="/js/gtm.js"></script><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><meta name="googlebot" content="NOODP" /><meta name="google-site-verification" content="nSYeDgyKM9mw5CWcZuD0xu7iSWXlJijAlg9rcxVOYf4" /><meta name="google-site-verification" content="6UEaC3SWhpGQvqRnSJIEm2swxXpM5Adn4dxZhFsNdw0" /><meta content="width=device-width, initial-scale=1.0, maximum-scale=1, minimum-scale=1, user-scalable=no" id="viewport" name="viewport" /><link href="https://fonts.googleapis.com/css?family=Raleway:400,500,600,700" rel="stylesheet" /><link rel="stylesheet" href="/css/rabbit.css" type="text/css" /><link rel="stylesheet" href="/css/highlightjs_style.css" type="text/css" /><link rel="stylesheet" href="/css/rabbit-next.css" type="text/css" /><!--[if IE 6]>
      <link rel="stylesheet" href="/css/rabbit-ie6.css" type="text/css" />
      <![endif]--><link rel="icon" type="/image/vnd.microsoft.icon" href="./favicon.ico" /><link rel="stylesheet" href="/css/tutorial.css" type="text/css" /><script async="true" type="text/javascript" src="/js/site.js"></script><title> Quorum Queues 
 â€” RabbitMQ</title></head>
  <body id="quorum-queues"><div id="outerContainer"><div class="container"><div class="rabbit-logo"><a href="/"><img src="/img/logo-rabbitmq.svg" alt="RabbitMQ" /></a></div><a class="btn menubtn" onclick="showHide()">Menu <img src="/img/carrot-down-white.svg" /></a><div class="mobilemenuicon" onclick="showHide()"><img src="/img/mobile-menu-icon.svg" /></div><div id="nav"><ul id="mainNav"><li><a href="/#features">Features</a></li><li><a href="/#getstarted">Get Started</a></li><li><a href="/#support">Support</a></li><li><a href="/#community">Community</a></li><li><a href="/documentation.html">Docs</a></li></ul></div></div><div class="nav-separator"></div><div id="innerContainer" class="container"><div id="left-content"><h1> Quorum Queues 
</h1>


<h2><a id="overview" class="anchor" href="#overview">Overview</a></h2>
<p>The quorum queue is a modern queue type for RabbitMQ implementing a durable,
replicated FIFO queue based on the <a href="https://raft.github.io/">Raft consensus algorithm</a>.
It is available as of RabbitMQ 3.8.0.</p>
<p>The quorum queue type is an alternative to durable <a href="ha.html">mirrored queues</a>
purpose built for a <a href="#use-cases">set of use cases</a> where <a href="#data-safety">data safety</a> is
a top priority. This is covered in <a href="#motivation">Motivation</a>.
Quorum queues should be considered the default option for a replicated queue type.</p>
<p>Quorum queues also have important <a href="#behaviour">differences in behaviour</a>
and some <a href="#feature-comparison">limitations</a> compared to classic mirrored queues,
including workload-specific ones, e.g. when consumers <a href="#repeated-requeues">repeatedly requeue the same message</a>.</p>
<p>Some features, such as <a href="#poison-message-handling">poison message handling</a>, are specific
to quorum queues.</p>
<p>For cases that would benefit from replication and repeatable reads, <a href="streams.html">streams</a> may
be a better option than quorum queues.</p>
<h2><a id="overview" class="anchor" href="#overview">Overview</a></h2>
<p>Topics covered in this guide include</p>
<ul>
<li><a href="#motivation">What are quorum queues</a> and why they were introduced</li>
<li><a href="#feature-comparison">How are they different</a> from classic queues</li>
<li>Primary <a href="#use-cases">use cases</a> of quorum queues and when not to use them</li>
<li>How to <a href="#usage">declare a quorum queue</a></li>
<li><a href="#replication">Replication</a>-related topics: replica management, <a href="#replica-rebalancing">replica leader rebalancing</a>, optimal number of replicas, etc</li>
<li>What guarantees quorum queues offer in terms of <a href="#leader-election">leader failure handling</a>, <a href="#data-safety">data safety</a> and <a href="#availability">availability</a></li>
<li><a href="#performance">Performance</a> characteristics</li>
<li><a href="#poison-message-handling">Poison message handling</a> provided by quorum queues</li>
<li><a href="#configuration">Configurable settings</a> of quorum queues</li>
<li>Resource use of quorum queues, most importantly their <a href="#resource-use">memory footprint</a></li>
</ul>
<p>and more.</p>
<p>This guide assumes general familiarity with <a href="/clustering.html">RabbitMQ clustering</a>.</p>
<h2><a id="motivation" class="anchor" href="#motivation">Motivation</a></h2>
<p>Quorum queues are designed to be safer and provide simpler, well defined failure handling semantics
that users should find easier to reason about when designing and operating their systems.</p>
<p>These design choices come with constraints. To reach this goal, quorum queues adopt a different replication
and consensus protocol and give up support for certain "transient" in nature features.
These constraints and limitations are covered later in this guide.</p>
<h3><a id="what-is-quorum" class="anchor" href="#what-is-quorum">What is a Quorum?</a></h3>
<p>If intentionally simplified, <a href="https://en.wikipedia.org/wiki/Quorum_(distributed_computing)">quorum</a> in a distributed system can
be defined as an agreement between the majority of nodes (<span class="code ">(N/2)+1</span> where <span class="code ">N</span> is the total number of
system participants).</p>
<p>When applied to queue mirroring in RabbitMQ <a href="/clustering.html">clusters</a>
this means that the majority of replicas (including the currently elected queue leader)
agree on the state of the queue and its contents.</p>
<h3>Differences from Classic Mirrored Queues</h3>
<p>Quorum queues share many of the fundamentals with <a href="queues.html">queues</a> of other types in RabbitMQ.
However, they are more purpose-built, focus on data safety and predictable recovery,
and do not support certain features.</p>
<p>The differences <a href="#feature-comparison">are covered</a> in this guide.</p>
<p>Classic mirrored queues in RabbitMQ have technical limitations that makes it difficult to provide
comprehensible guarantees and clear failure handling semantics.</p>
<p>Certain failure scenarios can result in mirrored queues
confirming messages too early, potentially resulting in a data loss.</p>
<h2><a id="feature-comparison" class="anchor" href="#feature-comparison">Feature Comparison with Regular Queues</a></h2>
<p>Quorum queues share most of the fundamentals with other <a href="queues.html">queue</a> types.
A client library that can use regular mirrored queues will be able to use quorum queues.</p>
<p>The following operations work the same way for quorum queues as they do for regular queues:</p>
<ul>
<li>Consumption (subscription)</li>
<li><a href="/confirms.html">Consumer acknowledgements</a> (except for global <a href="#global-qos">QoS and prefetch</a>)</li>
<li>Cancelling consumers</li>
<li>Purging</li>
<li>Deletion</li>
</ul>
<p>With some queue operations there are minor differences:</p>
<ul>
<li><a href="#declaring">Declaration</a></li>
<li>Setting prefetch for consumers</li>
</ul>
<p>Some features are not currently supported by quorum queues.</p>
<h3><a id="feature-matrix" class="anchor" href="#feature-matrix">Feature Matrix</a></h3>
<table>
<thead>
<tr>
<th align="left">Feature</th>
<th align="left">Classic Mirrored</th>
<th>Quorum</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><a href="queues.html">Non-durable queues</a></td>
<td align="left">yes</td>
<td>no</td>
</tr>
<tr>
<td align="left"><a href="queues.html">Exclusivity</a></td>
<td align="left">yes</td>
<td>no</td>
</tr>
<tr>
<td align="left">Per message persistence</td>
<td align="left">per message</td>
<td>always</td>
</tr>
<tr>
<td align="left">Membership changes</td>
<td align="left">automatic</td>
<td>manual</td>
</tr>
<tr>
<td align="left"><a href="/ttl.html">Message TTL (Time-To-Live)</a></td>
<td align="left">yes</td>
<td>yes (since 3.10)</td>
</tr>
<tr>
<td align="left"><a href="/ttl.html#queue-ttl">Queue TTL</a></td>
<td align="left">yes</td>
<td>yes</td>
</tr>
<tr>
<td align="left"><a href="/maxlength.html">Queue length limits</a></td>
<td align="left">yes</td>
<td>yes (except <span class="code ">x-overflow</span>: <span class="code ">reject-publish-dlx</span>)</td>
</tr>
<tr>
<td align="left"><a href="/lazy-queues.html">Lazy behaviour</a></td>
<td align="left">yes</td>
<td>yes through the <a href="#memory-limit">Memory Limit</a> feature (before 3.10); always (since 3.10)</td>
</tr>
<tr>
<td align="left"><a href="/priority.html">Message priority</a></td>
<td align="left">yes</td>
<td>no</td>
</tr>
<tr>
<td align="left"><a href="/consumer-priority.html">Consumer priority</a></td>
<td align="left">yes</td>
<td>yes</td>
</tr>
<tr>
<td align="left"><a href="/dlx.html">Dead letter exchanges</a></td>
<td align="left">yes</td>
<td>yes</td>
</tr>
<tr>
<td align="left">Adheres to <a href="/parameters.html#policies">policies</a></td>
<td align="left">yes</td>
<td>yes (see policy support below)</td>
</tr>
<tr>
<td align="left">Poison message handling</td>
<td align="left">no</td>
<td>yes</td>
</tr>
<tr>
<td align="left">Global <a href="#global-qos">QoS Prefetch</a></td>
<td align="left">yes</td>
<td>no</td>
</tr>
</tbody>
</table>
<h4>Non-durable Queues</h4>
<p>Regular queues can be <a href="queues.html">non-durable</a>. Quorum queues are always durable per their
assumed <a href="#use-cases">use cases</a>.</p>
<h4>Exclusivity</h4>
<p><a href="queues.html#exclusive-queues">Exclusive queues</a> are tied to the lifecycle of their declaring connection.
Quorum queues by design are replicated and durable, therefore the exclusive property makes
no sense in their context. Therefore quorum queues cannot be exclusive.</p>
<p>Quorum queues are not meant to be used as <a href="/queues.html#temporary-queues">temporary queues</a>.</p>
<h4>TTL (before RabbitMQ 3.10)</h4>
<p>Quorum queues support <a href="/ttl.html#queue-ttl">Queue TTL</a>, but do not support message TTL.</p>
<h4>TTL (since RabbitMQ 3.10)</h4>
<p>Quorum queues support both <a href="/ttl.html#queue-ttl">Queue TTL</a> and message TTL
(including <a href="/ttl.html#per-queue-message-ttl">Per-Queue Message TTL in Queues</a> and
<a href="/ttl.html#per-message-ttl-in-publishers">Per-Message TTL in Publishers</a>).
When using any form of message TTL, the memory overhead increases by 2 bytes per message.</p>
<h4>Length Limit</h4>
<p>Quorum queues has support for <a href="/maxlength.html">queue length limits</a>.</p>
<p>The <span class="code ">drop-head</span> and <span class="code ">reject-publish</span> overflow behaviours are supported but they
do not support <span class="code ">reject-publish-dlx</span> configurations as Quorum queues take a different
implementation approach than classic queues.</p>
<p>When a quorum queue reaches the max-length limit and <span class="code ">reject-publish</span> is configured
it notifies each publishing channel who from thereon will reject all messages back to
the client. This means that quorum queues may overshoot their limit by some small number
of messages as there may be messages in flight whilst the channels are notified.
The number of additional messages that are accepted by the queue will vary depending
on how many messages are in flight at the time.</p>
<h4><a id="dead-lettering" class="anchor" href="#dead-lettering">Dead Lettering</a></h4>
<p>Quorum queues support <a href="/dlx.html">dead letter exchanges</a> (DLXs).</p>
<p>Traditionally, using DLXs in a clustered environment has not been <a href="/dlx.html#safety">safe</a>.</p>
<p>Since RabbitMQ 3.10 quorum queues support a safer form of dead-lettering that uses
<span class="code ">at-least-once</span> guarantees for the message transfer between queues
(with the limitations and caveats outlined below).</p>
<p>This is done by implementing a special, internal dead-letter consumer process
that works similarly to a normal queue consumer with manual acknowledgements apart
from it only consumes messages that have been dead-lettered.</p>
<p>This means that the source quorum queue will retain the
dead-lettered messages until they have been acknowledged. The internal consumer
will consume dead-lettered messages and publish them to the target queue(s) using
publisher confirms. It will only acknowledge once publisher confirms have been
received, hence providing <span class="code ">at-least-once</span> guarantees.</p>
<p><span class="code ">at-most-once</span> remains the default dead-letter-strategy for quorum queues and is useful for scenarios
where the dead lettered messages are more of an informational nature and where it does not matter so much
if they are lost in transit between queues or when the overflow
configuration restriction outlined below is not suitable.</p>
<h5>Enabling at-least-once dead-lettering</h5>
<p>To enable <span class="code ">at-least-once</span> dead-lettering for a source quorum queue, apply all of the following policies
(or the equivalent queue arguments starting with <span class="code ">x-</span>):</p>
<ul>
<li>Set <span class="code ">dead-letter-strategy</span> to <span class="code ">at-least-once</span> (default is <span class="code ">at-most-once</span>).</li>
<li>Set <span class="code ">overflow</span> to <span class="code ">reject-publish</span> (default is <span class="code ">drop-head</span>).</li>
<li>Configure a <span class="code ">dead-letter-exchange</span>.</li>
<li>Enable <a href="/feature-flags.html">feature flag</a> <span class="code ">stream_queue</span> (enabled by default
for RabbitMQ clusters created in 3.9 or later).</li>
</ul>
<p>It is recommended to additionally configure <span class="code ">max-length</span> or <span class="code ">max-length-bytes</span>
to prevent excessive message buildup in the source quorum queue (see caveats below).</p>
<p>Optionally, configure a <span class="code ">dead-letter-routing-key</span>.</p>
<h5>Limitations</h5>
<p><span class="code ">at-least-once</span> dead lettering does not work with the default <span class="code ">drop-head</span> overflow
strategy even if a queue length limit is not set.
Hence if <span class="code ">drop-head</span> is configured the dead-lettering will fall back
to <span class="code ">at-most-once</span>. Use the overflow strategy <span class="code ">reject-publish</span> instead.</p>
<h5>Caveats</h5>
<p><span class="code ">at-least-once</span> dead-lettering will require more system resources such as memory and CPU.
Therefore, enable <span class="code ">at-least-once</span> only if dead lettered messages should not be lost.</p>
<p><span class="code ">at-least-once</span> guarantees opens up some specific failure cases that needs handling.
As dead-lettered messages are now retained by the source quorum queue until they have been
safely accepted by the dead-letter target queue(s) this means they have to contribute to the
queue resource limits, such as max length limits so that the queue can refuse to accept
more messages until some have been removed. Theoretically it is then possible for a queue
to <em>only</em> contain dead-lettered messages, in the case where, say a target dead-letter
queue isn't available to accept messages for a long time and normal queue consumers
consume most of the messages.</p>
<p>Dead-lettered messages are considered "live" until they have been confirmed
by the dead-letter target queue(s).</p>
<p>There are few cases for which dead lettered messages will not be removed
from the source queue in a timely manner:</p>
<ul>
<li>The configured dead-letter exchange does not exist.</li>
<li>The messages cannot be routed to any queue (equivalent to the <span class="code ">mandatory</span> message property).</li>
<li>One (of possibly many) routed target queues does not confirm receipt of the message.
This can happen when a target queue is not available or when a target queue rejects a message
(e.g. due to exceeded queue length limit).</li>
</ul>
<p>The dead-letter consumer process will retry periodically if either of the scenarios above
occur which means there is a possibility of duplicates appearing at the DLX target queue(s).</p>
<p>For each quorum queue with <span class="code ">at-least-once</span> dead-lettering enabled, there will be one internal dead-letter
consumer process. The internal dead-letter consumer process is co-located on the quorum queue leader node.
It keeps all dead-lettered message bodies in memory.
It uses a prefetch size of 32 messages to limit the amount of message bodies kept in memory if no confirms
are received from the target queues.</p>
<p>That prefetch size can be increased by the <span class="code ">dead_letter_worker_consumer_prefetch</span> setting in the <span class="code ">rabbit</span> app section of the
<a href="/configure.html#advanced-config-file">advanced config file</a> if high dead-lettering throughput
(thousands of messages per second) is required.</p>
<p>For a source quorum queue, it is possible to switch dead-letter strategy dynamically from <span class="code ">at-most-once</span>
to <span class="code ">at-least-once</span> and vice versa. If the dead-letter strategy is changed either directly
from <span class="code ">at-least-once</span> to <span class="code ">at-most-once</span> or indirectly, for example by changing overflow from <span class="code ">reject-publish</span>
to <span class="code ">drop-head</span>, any dead-lettered messages that have not yet been confirmed by all target queues will be deleted.</p>
<p>Messages published to the source quorum queue are persisted on disk regardless of the message delivery mode (transient or persistent).
However, messages that are dead lettered by the source quorum queue will keep the original message delivery mode.
This means if dead lettered messages in the target queue should survive a broker restart, the target queue must be durable and
the message delivery mode must be set to persistent when publishing messages to the source quorum queue.</p>
<h4>Lazy Mode (before RabbitMQ 3.10)</h4>
<p>Quorum queues store their content on disk (per Raft requirements) as well as in memory (up to the <a href="#memory-limit">in memory limit configured</a>).</p>
<p>The <a href="/lazy-queues.html#configuration"><span class="code ">lazy</span> mode configuration</a> does not apply.</p>
<p>It is possible to <a href="#memory-limit">limit how many messages a quorum queue keeps in memory</a> using a policy which
can achieve a behaviour similar to lazy queues.</p>
<h4>Lazy Mode (since RabbitMQ 3.10)</h4>
<p>Quorum queues store their message content on disk (per Raft requirements) and
only keep a small metadata record of each message in memory. This is a change from
prior versions of quorum queues where there was an option to keep the message bodies
in memory as well. This never proved to be beneficial especially when the queue length
was large.</p>
<p>The <a href="#memory-limit">memory limit</a> configuration is still permitted but has no
effect. The only option now is effectively the same as configuring: <span class="code ">x-max-in-memory-length=0</span></p>
<p>The <a href="/lazy-queues.html#configuration"><span class="code ">lazy</span> mode configuration</a> does not apply.</p>
<h4><a id="global-qos" class="anchor" href="#global-qos">Global QoS</a></h4>
<p>Quorum queues do not support global <a href="/confirms.html#channel-qos-prefetch">QoS prefetch</a> where a channel sets a single
prefetch limit for all consumers using that channel. If an attempt
is made to consume from a quorum queue from a channel with global QoS enabled
a channel error will be returned.</p>
<p>Use <a href="/consumer-prefetch.html">per-consumer QoS prefetch</a>, which is the default in several popular clients.</p>
<h4>Priorities</h4>
<p>Quorum queues do not currently support <a href="/priority.html">priorities</a>, including <a href="/consumer-priority.html">consumer priorities</a>.</p>
<p>To achieve priority processing with Quorum Queues multiple queues should be used instead;
one for each priority.</p>
<h4>Poison Message Handling</h4>
<p>Quorum queues <a href="#poison-message-handling">support poison message handling</a> via a redelivery limit.
This feature is currently unique to quorum queues.</p>
<h4>Policy Support</h4>
<p>Quorum queues can be configured via RabbitMQ policies. The below table summarises the
policy keys they adhere to.</p>
<table>
<thead>
<tr>
<th align="left">Definition Key</th>
<th align="left">Type</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><span class="code ">max-length</span></td>
<td align="left">Number</td>
</tr>
<tr>
<td align="left"><span class="code ">max-length-bytes</span></td>
<td align="left">Number</td>
</tr>
<tr>
<td align="left"><span class="code ">overflow</span></td>
<td align="left">"drop-head" or "reject-publish"</td>
</tr>
<tr>
<td align="left"><span class="code ">expires</span></td>
<td align="left">Number (milliseconds)</td>
</tr>
<tr>
<td align="left"><span class="code ">dead-letter-exchange</span></td>
<td align="left">String</td>
</tr>
<tr>
<td align="left"><span class="code ">dead-letter-routing-key</span></td>
<td align="left">String</td>
</tr>
<tr>
<td align="left"><span class="code ">max-in-memory-length</span></td>
<td align="left">Number</td>
</tr>
<tr>
<td align="left"><span class="code ">max-in-memory-bytes</span></td>
<td align="left">Number</td>
</tr>
<tr>
<td align="left"><span class="code ">delivery-limit</span></td>
<td align="left">Number</td>
</tr>
</tbody>
</table>
<h2><a id="use-cases" class="anchor" href="#use-cases">Use Cases</a></h2>
<p>Quorum queues are purpose built by design. They are <em>not</em> designed to be used for every problem.
Their intended use is for topologies where queues exist for a long time and are critical to certain
aspects of system operation, therefore fault tolerance and data safety is more important than, say,
lowest possible latency and advanced queue features.</p>
<p>Examples would be incoming orders in a sales system or votes cast in an
election system where potentially losing messages would have a significant
impact on system correctness and function.</p>
<p>Stock tickers and instant messaging systems benefit less or not at all from
quorum queues.</p>
<p>Publishers should use publisher confirms as this is how clients can interact with
the quorum queue consensus system. Publisher confirms will only be issued once
a published message has been successfully replicated to a quorum of nodes and is considered "safe"
within the context of the system.</p>
<p>Consumers should use manual acknowledgements to ensure messages that aren't
successfully processed are returned to the queue so that
another consumer can re-attempt processing.</p>
<h3>When Not to Use Quorum Queues</h3>
<p>In some cases quorum queues should not be used. They typically involve:</p>
<ul>
<li>Temporary nature of queues: transient or exclusive queues, high queue churn (declaration and deletion rates)</li>
<li>Lowest possible latency: the underlying consensus algorithm has an inherently higher latency due to its data safety features</li>
<li>When data safety is not a priority (e.g. applications do not use <a href="/confirms.html">manual acknowledgements and publisher confirms</a> are not used)</li>
<li>Very long queue backlogs (quorum queues currently keep all messages in memory at all times, up to a <a href="#memory-limit">limit</a>)</li>
</ul>
<h2><a id="usage" class="anchor" href="#usage">Usage</a></h2>
<p>As stated earlier, quorum queues share most of the fundamentals with other <a href="queues.html">queue</a> types.
A client library that can specify <a href="/queues.html#optional-arguments">optional queue arguments</a> will be able to use quorum queues.</p>
<p>First we will cover how to declare a quorum queue.</p>
<h3><a id="declaring" class="anchor" href="#declaring">Declaring</a></h3>
<p>To declare a quorum queue set the <span class="code ">x-queue-type</span> queue argument to <span class="code ">quorum</span>
(the default is <span class="code ">classic</span>). This argument must be provided by a client
at queue declaration time; it cannot be set or changed using a <a href="/parameters.html#policies">policy</a>.
This is because policy definition or applicable policy can be changed dynamically but
queue type cannot. It must be specified at the time of declaration.</p>
<p>Declaring a queue with an <span class="code ">x-queue-type</span> argument set to <span class="code ">quorum</span> will declare a quorum queue with
up to five replicas (default <a href="#replication-factor">replication factor</a>), one per each <a href="/clustering.html">cluster node</a>.</p>
<p>For example, a cluster of three nodes will have three replicas, one on each node.
In a cluster of seven nodes, five nodes will have one replica each but two nodes won't host any replicas.</p>
<p>After declaration a quorum queue can be bound to any exchange just as any other
RabbitMQ queue.</p>
<p>If declaring using <a href="/management.html">management UI</a>, queue type must be specified using
the queue type drop down menu.</p>
<h3>Client Operations</h3>
<p>The following operations work the same way for quorum queues as they do for classic queues:</p>
<ul>
<li><a href="/consumers.html">Consumption</a> (subscription)</li>
<li><a href="/confirms.html">Consumer acknowledgements</a> (keep <a href="#global-qos">QoS Prefetch Limitations</a> in mind)</li>
<li>Cancellation of consumers</li>
<li>Purging of queue messages</li>
<li>Queue deletion</li>
</ul>
<p>With some queue operations there are minor differences:</p>
<ul>
<li><a href="#declaring">Declaration</a> (covered above)</li>
<li>Setting <a href="#global-qos">QoS prefetch</a> for consumers</li>
</ul>
<h2><a id="replication" class="anchor" href="#replication">Replication and Data Locality</a></h2>
<p>When a quorum queue is declared, an initial number of replicas for it must be started in the cluster.
By default the number of replicas to be started is up to three, one per RabbitMQ node in the cluster.</p>
<p>Three nodes is the <strong>practical minimum</strong> of replicas for a quorum queue. In RabbitMQ clusters with a larger
number of nodes, adding more replicas than a <a href="#what-is-quorum">quorum</a> (majority) will not provide
any improvements in terms of <a href="#quorum-requirements">quorum queue availability</a> but it will consume
more cluster resources.</p>
<p>Therefore the <strong>recommended number of replicas</strong> for a quorum queue is the quorum of cluster nodes
(but no fewer than three). This assumes a <a href="cluster-formation.html">fully formed</a> cluster of at least three nodes.</p>
<h3><a id="replication-factor" class="anchor" href="#replication-factor">Controlling the Initial Replication Factor</a></h3>
<p>For example, a cluster of three nodes will have three replicas, one on each node.
In a cluster of seven nodes, three nodes will have one replica each but four more nodes won't host any replicas
of the newly declared queue.</p>
<p>Like with classic mirrored queues, the replication factor (number of replicas a queue has) can be configured for quorum queues.</p>
<p>The minimum factor value that makes practical sense is three.
It is highly recommended for the factor to be an odd number.
This way a clear quorum (majority) of nodes can be computed. For example, there is no "majority" of
nodes in a two node cluster. This is covered with more examples below in the <a href="#quorum-requirements">Fault Tolerance and Minimum Number of Replicas Online</a>
section.</p>
<p>This may not be desirable for larger clusters or for cluster with an even number of
nodes. To control the number of quorum queue members set the
<span class="code ">x-quorum-initial-group-size</span> queue argument when declaring the queue. The
group size argument provided should be an integer that is greater than zero and smaller or
equal to the current RabbitMQ cluster size. The quorum queue will be
launched to run on a random subset of RabbitMQ nodes present in the cluster at declaration time.</p>
<p>In case a quorum queue is declared before all cluster nodes have joined the cluster, and the initial replica
count is greater than the total number of cluster members, the effective value used will
be equal to the total number of cluster nodes. When more nodes join the cluster, the replica count
will not be automatically increased but it can be <a href="#replica-management">increased by the operator</a>.</p>
<h3><a id="leader-placement" class="anchor" href="#leader-placement">Queue Leader Location</a></h3>
<p>Every quorum queue has a primary replica. That replica is called
<em>queue leader</em>. All queue operations go through the leader
first and then are replicated to followers (mirrors). This is necessary to
guarantee FIFO ordering of messages.</p>
<p>To avoid some nodes in a cluster hosting the majority of queue leader
replicas and thus handling most of the load, queue leaders should
be reasonably evenly distributed across cluster nodes.</p>
<p>When a new quorum queue is declared, the set of nodes that will host its
replicas is randomly picked, but will always include the node the client that declares the queue is connected to.</p>
<p>Which replica becomes the initial leader can controlled using three options:</p>
<ol>
<li>Setting the <span class="code ">queue-leader-locator</span> <a href="parameters.html#policies">policy</a> key (recommended)</li>
<li>By defining the <span class="code ">queue_leader_locator</span> key in <a href="configure.html#configuration-files">the configuration file</a> (recommended)</li>
<li>Using the <span class="code ">x-queue-leader-locator</span> <a href="queues.html#optional-arguments">optional queue argument</a></li>
</ol>
<p>Supported queue leader locator values are</p>
<ul>
<li><span class="code ">client-local</span>: Pick the node the client that declares the queue is connected to. This is the default value.</li>
<li><span class="code ">balanced</span>: If there are overall less than 1000 queues (classic queues, quorum queues, and streams),
   pick the node hosting the minimum number of quorum queue leaders.
   If there are overall more than 1000 queues, pick a random node.</li>
</ul>
<h3><a id="replica-management" class="anchor" href="#replica-management">Managing Replicas</a> (Quorum Group Members)</h3>
<p>Replicas of a quorum queue are explicitly managed by the operator. When a new node is added
to the cluster, it will host no quorum queue replicas unless the operator explicitly adds it
to a member (replica) list of a quorum queue or a set of quorum queues.</p>
<p>When a node has to be decommissioned (permanently removed from the cluster), it must be explicitly
removed from the member list of all quorum queues it currently hosts replicas for.</p>
<p>Several <a href="/cli.html">CLI commands</a> are provided to perform the above operations:</p>
<pre class="lang-bash">
rabbitmq-queues add_member [-p &lt;vhost&gt;] &lt;queue-name&gt; &lt;node&gt;
</pre>

<pre class="lang-bash">
rabbitmq-queues delete_member [-p &lt;vhost&gt;] &lt;queue-name&gt; &lt;node&gt;
</pre>

<pre class="lang-bash">
rabbitmq-queues grow &lt;node&gt; &lt;all | even&gt; [--vhost-pattern &lt;pattern&gt;] [--queue-pattern &lt;pattern&gt;]
</pre>

<pre class="lang-bash">
rabbitmq-queues shrink &lt;node&gt; [--errors-only]
</pre>

<p>To successfully add and remove members a quorum of replicas in the cluster must be available
because cluster membership changes are treated as queue state changes.</p>
<p>Care needs to be taken not to accidentally make a queue unavailable by losing
the quorum whilst performing maintenance operations that involve membership changes.</p>
<p>When replacing a cluster node, it is safer to first add a new node and then decomission the node
it replaces.</p>
<h3><a id="replica-rebalancing" class="anchor" href="#replica-rebalancing">Rebalancing Replicas</a></h3>
<p>Once declared, the RabbitMQ nodes a quorum queue resides on won't change even if the
members of the RabbitMQ cluster change (e.g. a node is decommissioned or added).
To re-balance after a RabbitMQ cluster change quorum queues will have to be manually adjusted using the <span class="code ">rabbitmq-queues</span>
<a href="/cli.html">command line tool</a>:</p>
<pre class="lang-bash">
# rebalances all quorum queues
rabbitmq-queues rebalance quorum
</pre>

<p>it is possible to rebalance a subset of queues selected by name:</p>
<pre class="lang-bash">
# rebalances a subset of quorum queues
rabbitmq-queues rebalance quorum --queue-pattern "orders.*"
</pre>

<p>or quorum queues in a particular set of virtual hosts:</p>
<pre class="lang-bash">
# rebalances a subset of quorum queues
rabbitmq-queues rebalance quorum --vhost-pattern "production.*"
</pre>

<h2><a id="behaviour" class="anchor" href="#behaviour">Behaviour</a></h2>
<p>A quorum queue relies on a consensus protocol called Raft to ensure data consistency and safety.</p>
<p>Every quorum queue has a primary replica (a <em>leader</em> in Raft parlance) and zero or more
secondary replicas (called <em>followers</em>).</p>
<p>A leader is elected when the cluster is first formed and later if the leader
becomes unavailable.</p>
<h3><a id="leader-election" class="anchor" href="#leader-election">Leader Election and Failure Handling</a></h3>
<p>A quorum queue requires a quorum of the declared nodes to be available
to function. When a RabbitMQ node hosting a quorum queue's
<em>leader</em> fails or is stopped another node hosting one of that
quorum queue's <em>follower</em> will be elected leader and resume
operations.</p>
<p>Failed and rejoining followers will re-synchronise ("catch up") with the leader.
In contrast to classic mirrored queues, a temporary replica failure
does not require a full re-synchronization from the currently elected leader. Only the delta
will be transferred if a re-joining replica is behind the leader. This "catching up" process
does not affect leader availability.</p>
<p>Except for the initial replica set selection, replicas must be explicitly added to a quorum queue.
When a new replica is <a href="#replica-management">added</a>, it will synchronise the entire queue state
from the leader, similarly to classic mirrored queues.</p>
<h3><a id="quorum-requirements" class="anchor" href="#quorum-requirements">Fault Tolerance and Minimum Number of Replicas Online</a></h3>
<p>Consensus systems can provide certain guarantees with regard to data safety.
These guarantees do mean that certain conditions need to be met before they
become relevant such as requiring a minimum of three cluster nodes to provide
fault tolerance and requiring more than half of members to be available to
work at all.</p>
<p>Failure tolerance characteristics of clusters of various size can be described
in a table:</p>
<table>
<thead>
<tr>
<th align="center">Cluster node count</th>
<th align="center">Tolerated number of node failures</th>
<th align="center">Tolerant to a network partition</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">1</td>
<td align="center">0</td>
<td align="center">not applicable</td>
</tr>
<tr>
<td align="center">2</td>
<td align="center">0</td>
<td align="center">no</td>
</tr>
<tr>
<td align="center">3</td>
<td align="center">1</td>
<td align="center">yes</td>
</tr>
<tr>
<td align="center">4</td>
<td align="center">1</td>
<td align="center">yes if a majority exists on one side</td>
</tr>
<tr>
<td align="center">5</td>
<td align="center">2</td>
<td align="center">yes</td>
</tr>
<tr>
<td align="center">6</td>
<td align="center">2</td>
<td align="center">yes if a majority exists on one side</td>
</tr>
<tr>
<td align="center">7</td>
<td align="center">3</td>
<td align="center">yes</td>
</tr>
<tr>
<td align="center">8</td>
<td align="center">3</td>
<td align="center">yes if a majority exists on one side</td>
</tr>
<tr>
<td align="center">9</td>
<td align="center">4</td>
<td align="center">yes</td>
</tr>
</tbody>
</table>
<p>As the table above shows RabbitMQ clusters with fewer than three nodes do not
benefit fully from the quorum queue guarantees. RabbitMQ clusters with an even
number of RabbitMQ nodes do not benefit from having quorum queue members spread
over all nodes. For these systems the quorum queue size should be constrained to a
smaller uneven number of nodes.</p>
<p>Performance tails off quite a bit for quorum queue node sizes larger than 5.
We do not recommend running quorum queues on more than 7 RabbitMQ nodes. The
default quorum queue size is 3 and is controllable using the
<span class="code ">x-quorum-initial-group-size</span> <a href="/queues.html#optional-arguments">queue argument</a>.</p>
<h3><a id="data-safety" class="anchor" href="#data-safety">Data Safety</a></h3>
<p>Quorum queues are designed to provide data safety under network partition and
failure scenarios. A message that was successfully confirmed back to the publisher
using the <a href="confirms.html">publisher confirms</a> feature should not be lost as long as at
least a majority of RabbitMQ nodes hosting the quorum queue are not
permanently made unavailable.</p>
<p>Generally quorum queues favours data consistency over availability.</p>
<p><em><em>No guarantees are provided for messages that have not been confirmed using
the publisher confirm mechanism</em></em>. Such messages could be lost "mid-way", in an operating
system buffer or otherwise fail to reach the queue leader.</p>
<h3><a id="availability" class="anchor" href="#availability">Availability</a></h3>
<p>A quorum queue should be able to tolerate a minority of queue members becoming unavailable
with no or little effect on availability.</p>
<p>Note that depending on the <a href="/partitions.html">partition handling strategy</a>
used RabbitMQ may restart itself during recovery and reset the node but as long as that
does not happen, this availability guarantee should hold true.</p>
<p>For example, a queue with three replicas can tolerate one node failure without losing availability.
A queue with five replicas can tolerate two, and so on.</p>
<p>If a quorum of nodes cannot be recovered (say if 2 out of 3 RabbitMQ nodes are
permanently lost) the queue is permanently unavailable and
will need to be force deleted and recreated.</p>
<p>Quorum queue follower replicas that are disconnected from the leader or participating in a leader
election will ignore queue operations sent to it until they become aware of a newly elected leader.
There will be warnings in the log (<span class="code ">received unhandled msg</span> and similar) about such events.
As soon as the replica discovers a newly elected leader, it will sync the queue operation
log entries it does not have from the leader, including the dropped ones. Quorum queue state
will therefore remain consistent.</p>
<h2><a id="performance" class="anchor" href="#performance">Performance Characteristics</a></h2>
<p>Quorum queues are designed to trade latency for throughput and have been tested
and compared against durable <a href="/ha.html">classic mirrored queues</a> in 3, 5 and 7 node configurations at several
message sizes. In scenarios using both consumer acks and publisher confirms
 quorum queues have been observed to have equal or greater throughput to
classic mirrored queues.</p>
<p>As quorum queues persist all data to disks before doing anything it is recommended
to use the fastest disks possible. Quorum queues also benefit from consumers
using higher prefetch values to ensure consumers aren't starved whilst
acknowledgements are flowing through the system and allowing messages
to be delivered in a timely fashion.</p>
<p>Due to the disk I/O-heavy nature of quorum queues, their throughput decreases
as message sizes increase.</p>
<p>Just like mirrored queues, quorum queues are also affected by cluster sizes.
The more replicas a quorum queue has, the lower its throughput generally will
be since more work has to be done to replicate data and achieve consensus.</p>
<h2><a id="configuration" class="anchor" href="#configuration">Configuration</a></h2>
<p>There are a few new configuration parameters that can be tweaked using
the <a href="configure.html#advanced-config-file">advanced</a> config file.</p>
<p>Note that all settings related to <a href="#resource-use">resource footprint</a> are documented
in a separate section.</p>
<p>The <span class="code ">ra</span> application (which is the Raft library that quorum
queues use) has <a href="https://github.com/rabbitmq/ra#configuration">its own set of tunable parameters</a>.</p>
<p>The <span class="code ">rabbit</span> application has several quorum queue related configuration items available.</p>
<table>
  <thead>
    <tr>
      <td><span class="code ">advanced.config</span> Configuration Key</td>
      <td>Description</td>
      <td>Default value</td>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>rabbit.quorum_cluster_size</td>
      <td>
        Sets the default quorum queue cluster size (can be over-ridden by the <span class="code ">x-quorum-initial-group-size</span>
        queue argument at declaration time.
      </td>
      <td>3</td>
    </tr>
    <tr>
      <td>rabbit.quorum_commands_soft_limit</td>
      <td>
        This is a flow control related parameter defining
        the maximum number of unconfirmed messages a channel accepts before entering flow.
        The current default is configured to provide good performance and stability
        when there are multiple publishers sending to the same quorum queue. If the applications
        typically only have a single publisher per queue this limit could be increased to provide
        somewhat better ingress rates.
      </td>
      <td>32</td>
    </tr>
  </tbody>
</table>

<h3>Example</h3>
<p>The following <span class="code ">advanced.config</span> example modifies all values listed above:</p>
<pre class="lang-erlang">
[
 %% five replicas by default, only makes sense for nine node clusters
 {rabbit, [{quorum_cluster_size, 5},
           {quorum_commands_soft_limit, 512}]}
]
</pre>

<h2><a id="poison-message-handling" class="anchor" href="#poison-message-handling">Poison Message Handling</a></h2>
<p>Quorum queue support handling of <a href="https://en.wikipedia.org/wiki/Poison_message">poison messages</a>,
that is, messages that cause a consumer to repeatedly requeue a delivery (possibly due to a consumer failure)
such that the message is never consumed completely and <a href="/confirms.html">positively acknowledged</a> so that it can be marked for
deletion by RabbitMQ.</p>
<p>Quorum queues keep track of the number of unsuccessful delivery attempts and expose it in the
"x-delivery-count" header that is included with any redelivered message.</p>
<p>It is possible to set a delivery limit for a queue using a <a href="/parameters.html#policies">policy</a> argument, <span class="code ">delivery-limit</span>.</p>
<p>When a message has been returned more times than the limit the message will be dropped or
<a href="/dlx.html">dead-lettered</a> (if a DLX is configured).</p>
<h2><a id="resource-use" class="anchor" href="#resource-use">Resource Use</a></h2>
<p>Quorum queues typically require more resources (disk and RAM)
than classic mirrored queues. To enable fast election of a new leader and recovery, data safety as well as
good throughput characteristics all members in a quorum queue
"cluster" keep all messages in the queue in memory <em>and</em> on disk.</p>
<p>Quorum queues use a write-ahead-log (WAL) for all operations.
WAL operations are stored both in memory and written to disk.
When the current WAL file reaches a predefined limit, it is flushed to a WAL segment file on disk
and the system will begin to release the memory used by that batch of log entries.
The segment files are then compacted over time as consumers <a href="/confirms.html">acknowledge deliveries</a>.
Compaction is the process that reclaims disk space.</p>
<p>The WAL file size limit at which it is flushed to disk can be controlled:</p>
<pre class="lang-ini">
# Flush current WAL file to a segment file on disk once it reaches 64 MiB in size
raft.wal_max_size_bytes = 64000000
</pre>

<p>The value defaults to 512 MiB. This means that during steady load, the WAL table memory
footprint can reach 512 MiB.</p>
<p>Because memory deallocation may take some time,
we recommend that the RabbitMQ node is allocated at least 3 times the memory of the default WAL file size limit.
More will be required in high-throughput systems. 4 times is a good starting point for those.</p>
<h3><a id="memory-limit" class="anchor" href="#memory-limit">Configuring Per Queue Memory Limit</a></h3>
<p>Before RabbitMQ 3.10 it was possible to limit the amount of memory each quorum queue will use for the part of its log that
is kept in memory. Note that these limits are different from those of the <a href="#resource-use">in-memory Raft WAL table</a>
and <a href="/maxlength.html">queue length limits</a>.</p>
<p>The limit is controlled using <a href="/queues.html#optional-arguments">optional queue arguments</a>
that are best configured using a <a href="/parameters.html#policies">policy</a>.</p>
<ul>
<li><span class="code ">x-max-in-memory-length</span> sets a limit as a number of messages. Must be a non-negative integer.</li>
<li><span class="code ">x-max-in-memory-bytes</span> sets a limit as the total size of message bodies (payloads), in bytes. Must be a non-negative integer.</li>
</ul>
<p>Since RabbitMQ 3.10 these settings are deprecated.
They can still be set but have no effect.
The new behaviour is effectively the same as setting <span class="code ">x-max-in-memory-length=0</span> keeping no message bodies in memory.</p>
<h3><a id="repeated-requeues" class="anchor" href="#repeated-requeues">Repeated Requeues</a></h3>
<p>Internally quorum queues are implemented using a log where all operations including
messages are persisted. To avoid this log growing too large it needs to be
truncated regularly. To be able to truncate a section of the log all messages
in that section needs to be acknowledged. Usage patterns that continuously
<a href="/nack.html">reject or nack</a> the same message with the <span class="code ">requeue</span> flag set to true
could cause the log to grow in an unbounded fashion and eventually fill
up the disks.</p>
<p>Since RabbitMQ 3.10 messages that are rejected or nacked back to a quorum queue will be
returned to the <em>back</em> of the queue <em>if</em> no <a href="#poison-message-handling">delivery-limit</a> is set. This avoids
the above scenario where repeated re-queues causes the Raft log to grow in an
unbounded manner. If a <span class="code ">delivery-limit</span> is set it will use the original behaviour
of returning the message near the head of the queue.</p>
<h3><a id="atom-use" class="anchor" href="#atom-use">Increased Atom Use</a></h3>
<p>The internal implementation of quorum queues converts the queue name
into an Erlang atom. If queues with arbitrary names are continuously
created and deleted it <em>may</em> threaten the long term stability of the
RabbitMQ system (if the size of the atom table reaches the maximum limit,
about 1M by default). It is not recommended to use quorum queues in this manner
at this point.</p><div id="help-and-feedback"><h2>Getting Help and Providing Feedback</h2><p>
                    If you have questions about the contents of this guide or
                    any other topic related to RabbitMQ, don't hesitate to ask them
                    on the <a href="https://groups.google.com/forum/#!forum/rabbitmq-users">RabbitMQ mailing list</a>.
                  </p></div><div id="contribute"><h2>Help Us Improve the Docs &lt;3</h2><p>
                    If you'd like to contribute an improvement to the site,
                    its source is <a href="https://github.com/rabbitmq/rabbitmq-website">available on GitHub</a>.
                    Simply fork the repository and submit a pull request. Thank you!
                  </p></div></div><div id="right-nav"></div></div><div class="clear"></div><div class="pageFooter"><div class="container"></div><div class="container"><div class="rabbit-logo"><a href="/"><img src="/img/logo-rabbitmq-white.svg" alt="RabbitMQ" /></a></div><ul class="footerNav"><li><a href="/#features">Features</a></li><li><a href="/#getstarted">Get Started</a></li><li><a href="/#support">Support</a></li><li><a href="/#community">Community</a></li><li><a href="/documentation.html">Docs</a></li></ul><p id="copyright">
          Copyright Â© 2007-2022 <a href="https://tanzu.vmware.com/">VMware</a>, Inc. or its affiliates. All rights reserved.
          <a href="https://www.vmware.com/help/legal.html">Terms of Use</a> â€¢
          <a href="https://www.vmware.com/help/privacy.html">Privacy</a> â€¢
          <a href="/trademark-guidelines.html">Trademark Guidelines</a> â€¢
          <a href="https://www.vmware.com/help/privacy/california-privacy-rights.html">Your California Privacy Rights</a> â€¢
          <a class="ot-sdk-show-settings">Cookie Settings</a><br /><a id="teconsent"></a></p></div></div></div><script type="text/javascript" src="/js/highlight.pack.js"></script><script type="text/javascript">
        // code highlighting
        window.addEventListener("load", function() {
          const selectors = "pre.lang-bash, \
                             pre.lang-csharp, \
                             pre.lang-elixir, \
                             pre.lang-erlang, \
                             pre.lang-go, \
                             pre.lang-groovy, \
                             pre.lang-haskell, \
                             pre.lang-html, \
                             pre.lang-ini, \
                             pre.lang-java, \
                             pre.lang-javascript, \
                             pre.lang-json, \
                             pre.lang-makefile, \
                             pre.lang-objectivec, \
                             pre.lang-php, \
                             pre.lang-plaintext, \
                             pre.lang-powershell, \
                             pre.lang-python, \
                             pre.lang-ruby, \
                             pre.lang-swift, \
                             pre.lang-yaml, \
                             pre.lang-xml";
          document.querySelectorAll(selectors).forEach(function(el) {
            hljs.highlightBlock(el);
          });
        });
      </script></body>
</html>
