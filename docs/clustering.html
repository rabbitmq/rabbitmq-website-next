<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
      new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
      j=d.createElement(s),dl=l!='dataLayer'?'&amp;l='+l:'';j.async=true;j.src=
      'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
      })(window,document,'script','dataLayer','GTM-NSPM4RC');</script><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><meta name="googlebot" content="NOODP" /><meta name="google-site-verification" content="nSYeDgyKM9mw5CWcZuD0xu7iSWXlJijAlg9rcxVOYf4" /><meta name="google-site-verification" content="6UEaC3SWhpGQvqRnSJIEm2swxXpM5Adn4dxZhFsNdw0" /><meta content="width=device-width, initial-scale=1.0, maximum-scale=1, minimum-scale=1, user-scalable=no" id="viewport" name="viewport" /><link href="https://fonts.googleapis.com/css?family=Raleway:400,500,600,700" rel="stylesheet" /><link rel="stylesheet" href="/css/rabbit.css" type="text/css" /><link rel="stylesheet" href="/css/highlightjs_style.css" type="text/css" /><link rel="stylesheet" href="/css/rabbit-next.css" type="text/css" /><!--[if IE 6]>
      <link rel="stylesheet" href="/css/rabbit-ie6.css" type="text/css" />
      <![endif]--><link rel="icon" type="/image/vnd.microsoft.icon" href="/favicon.ico" /><link rel="stylesheet" href="/css/tutorial.css" type="text/css" /><script async="true" type="text/javascript" src="/js/site.js"></script><title> Clustering Guide
 â€” RabbitMQ</title></head>
  <body id="clustering"><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NSPM4RC" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="outerContainer"><div class="container"><div class="rabbit-logo"><a href="/"><img src="/img/logo-rabbitmq.svg" alt="RabbitMQ" /></a></div><a class="btn menubtn" onclick="showHide()">Menu <img src="/img/carrot-down-white.svg" /></a><div class="mobilemenuicon" onclick="showHide()"><img src="/img/mobile-menu-icon.svg" /></div><div id="nav"><ul id="mainNav"><li><a href="/#features">Features</a></li><li><a href="/#getstarted">Get Started</a></li><li><a href="/#support">Support</a></li><li><a href="/#community">Community</a></li><li><a href="/documentation.html">Docs</a></li></ul></div></div><div class="nav-separator"></div><div id="innerContainer" class="container"><div id="left-content"><h1> Clustering Guide
</h1>


<h2><a id="overview" class="anchor" href="#overview">Overview</a></h2>
<p>This guide covers fundamental topics related to RabbitMQ clustering:</p>
<ul>
<li>How RabbitMQ nodes are identified: <a href="#node-names">node names</a></li>
<li><a href="#cluster-formation-requirements">Requirements</a> for clustering</li>
<li>What data is and isn't <a href="#cluster-membership">replicated between cluster nodes</a></li>
<li>What clustering <a href="#clustering-and-clients">means for clients</a></li>
<li><a href="#cluster-formation">How clusters are formed</a></li>
<li>How nodes <a href="#erlang-cookie">authenticate to each other</a> (and with CLI tools)</li>
<li>Why it's important to <a href="#node-count">use an odd number of nodes</a> and <strong>two cluster nodes are highly recommended against</strong></li>
<li><a href="#restarting">Node restarts</a> and how nodes rejoin their cluster</li>
<li><a href="#restarting-readiness-probes">Node readiness probes</a> and how they can affect rolling cluster restarts</li>
<li>How to <a href="#removing-nodes">remove a cluster node</a></li>
<li>How to <a href="#resetting-nodes">reset a cluster node</a> to a pristine (blank) state</li>
</ul>
<p>and more. <a href="/cluster-formation.html">Cluster Formation and Peer Discovery</a> is a closely related guide
that focuses on peer discovery and cluster formation automation-related topics. For queue contents
(message) replication, see the <a href="/quorum-queues.html">Quorum Queues</a> guide.</p>
<p><a href="tanzu">Tanzu RabbitMQ</a> provides an <a href="clustering-compression.html">inter-node traffic compression</a> feature.</p>
<p>A RabbitMQ cluster is a logical grouping of one or
several nodes, each  sharing users, virtual hosts,
queues, exchanges, bindings, runtime parameters and other distributed state.</p>
<h2><a id="cluster-formation" class="anchor" href="#cluster-formation">Cluster Formation</a></h2>
<h3><a id="cluster-formation-options" class="anchor" href="#cluster-formation-options">Ways of Forming a Cluster</a></h3>
<p>A RabbitMQ cluster can formed in a number of ways:</p>
<ul>
<li>Declaratively by listing cluster nodes in <a href="/configure.html">config file</a></li>
<li>Declaratively using DNS-based discovery</li>
<li>Declaratively using <a href="https://github.com/rabbitmq/rabbitmq-peer-discovery-aws">AWS (EC2) instance discovery</a> (via a plugin)</li>
<li>Declaratively using <a href="https://github.com/rabbitmq/rabbitmq-peer-discovery-k8s">Kubernetes discovery</a> (via a plugin)</li>
<li>Declaratively using <a href="https://github.com/rabbitmq/rabbitmq-peer-discovery-consul">Consul-based discovery</a> (via a plugin)</li>
<li>Declaratively using <a href="https://github.com/rabbitmq/rabbitmq-peer-discovery-etcd">etcd-based discovery</a> (via a plugin)</li>
<li>Manually with <span class="code ">rabbitmqctl</span></li>
</ul>
<p>Please refer to the <a href="/cluster-formation.html">Cluster Formation guide</a> for details.</p>
<p>The composition of a cluster can be altered dynamically.
All RabbitMQ brokers start out as running on a single
node. These nodes can be joined into clusters, and
subsequently turned back into individual brokers again.</p>
<h3><a id="node-names" class="anchor" href="#node-names">Node Names (Identifiers)</a></h3>
<p>RabbitMQ nodes are identified by node names. A node name consists of two parts,
a prefix (usually <span class="code ">rabbit</span>) and hostname. For example, <span class="code ">rabbit@node1.messaging.svc.local</span>
is a node name with the prefix of <span class="code ">rabbit</span> and hostname of <span class="code ">node1.messaging.svc.local</span>.</p>
<p>Node names in a cluster must be unique. If more than one node is running on a given host
(this is usually the case in development and QA environments), they must use
different prefixes, e.g. <span class="code ">rabbit1@hostname</span> and <span class="code ">rabbit2@hostname</span>.</p>
<p>In a cluster, nodes identify and contact each other using node names. This means
that the hostname part of every node name <a href="#hostname-resolution-requirement">must resolve</a>.
<a href="/cli.html">CLI tools</a> also identify and address nodes using node names.</p>
<p>When a node starts up, it checks whether it has been assigned a node name. This is done
via the <span class="code ">RABBITMQ_NODENAME</span> <a href="/configure.html#supported-environment-variables">environment variable</a>.
If no value was explicitly configured,
the node resolves its hostname and prepends <span class="code ">rabbit</span> to it to compute its node name.</p>
<p>If a system uses fully qualified domain names (FQDNs) for hostnames, RabbitMQ nodes
and CLI tools must be configured to use so called long node names.
For server nodes this is done by setting the <span class="code ">RABBITMQ_USE_LONGNAME</span> <a href="/configure.html#supported-environment-variables">environment variable</a>
to <span class="code ">true</span>.</p>
<p>For CLI tools, either <span class="code ">RABBITMQ_USE_LONGNAME</span> must be set or the <span class="code ">--longnames</span> option
must be specified.</p>
<h2><a id="cluster-formation-requirements" class="anchor" href="#cluster-formation-requirements">Cluster Formation Requirements</a></h2>
<h3><a id="hostname-resolution-requirement" class="anchor" href="#hostname-resolution-requirement">Hostname Resolution</a></h3>
<p>RabbitMQ nodes address each other using domain names,
either short or fully-qualified (FQDNs). Therefore
hostnames of all cluster members
must be resolvable from all cluster nodes, as well
as machines on which command line tools such as <span class="code ">rabbitmqctl</span>
might be used.</p>
<p>Hostname resolution can use any of the standard OS-provided
methods:</p>
<ul>
<li>DNS records</li>
<li>Local host files (e.g. <span class="code ">/etc/hosts</span>)</li>
</ul>
<p>In more restrictive environments, where DNS record or
hosts file modification is restricted, impossible or
undesired, <a href="http://erlang.org/doc/apps/erts/inet_cfg.html">Erlang
VM can be configured to use alternative hostname
resolution methods</a>, such as an alternative DNS server,
a local file, a non-standard hosts file location, or a mix
of methods.  Those methods can work in concert with the
standard OS hostname resolution methods.</p>
<p>To use FQDNs, see <span class="code ">RABBITMQ_USE_LONGNAME</span> in the <a href="/configure.html#supported-environment-variables">Configuration guide</a>.
See <a href="#node-names">Node Names</a> above.</p>
<h2><a id="ports" class="anchor" href="#ports">Port Access</a></h2>
<p>RabbitMQ nodes bind to ports (open server TCP sockets) in order to accept client and CLI tool connections.
Other processes and tools such as SELinux may prevent RabbitMQ from binding to a port. When that happens,
the node will fail to start.</p>
<p>CLI tools, client libraries and RabbitMQ nodes also open connections (client TCP sockets).
Firewalls can prevent nodes and CLI tools from communicating with each other.
Make sure the following ports are accessible:</p>
<ul>
<li>4369: <a href="/networking.html#epmd">epmd</a>, a helper discovery daemon used by RabbitMQ nodes and CLI tools</li>
<li>5672, 5671: used by AMQP 0-9-1 and 1.0 clients without and with TLS</li>
<li>25672: used for inter-node and CLI tools communication (Erlang distribution server port)
   and is allocated from a dynamic range (limited to a single port by default,
   computed as AMQP port + 20000). Unless external connections on these ports are really necessary (e.g.
   the cluster uses <a href="/federation.html">federation</a> or CLI tools are used on machines outside the subnet),
   these ports should not be publicly exposed. See <a href="/networking.html">networking guide</a> for details.</li>
<li>35672-35682: used by CLI tools (Erlang distribution client ports) for communication with nodes
   and is allocated from a dynamic range (computed as server distribution port + 10000 through
   server distribution port + 10010). See <a href="/networking.html">networking guide</a> for details.</li>
<li>15672: <a href="/management.html">HTTP API</a> clients, <a href="/management.html">management UI</a> and <a href="/management-cli.html">rabbitmqadmin</a>
   (only if the <a href="/management.html">management plugin</a> is enabled)</li>
<li>61613, 61614: <a href="https://stomp.github.io/stomp-specification-1.2.html">STOMP clients</a> without and with TLS (only if the <a href="/stomp.html">STOMP plugin</a> is enabled)</li>
<li>1883, 8883: <a href="http://mqtt.org/">MQTT clients</a> without and with TLS, if the <a href="/mqtt.html">MQTT plugin</a> is enabled</li>
<li>15674: STOMP-over-WebSockets clients (only if the <a href="/web-stomp.html">Web STOMP plugin</a> is enabled)</li>
<li>15675: MQTT-over-WebSockets clients (only if the <a href="/web-mqtt.html">Web MQTT plugin</a> is enabled)</li>
<li>15692: Prometheus metrics (only if the <a href="/prometheus.html">Prometheus plugin</a> is enabled)</li>
</ul>
<p>It is possible to <a href="/configure.html">configure RabbitMQ</a>
to use <a href="/networking.html">different ports and specific network interfaces</a>.</p>
<h2><a id="cluster-membership" class="anchor" href="#cluster-membership">Nodes in a Cluster</a></h2>
<h3><a id="overview-what-is-replicated" class="anchor" href="#overview-what-is-replicated">What is Replicated?</a></h3>
<p>All data/state required for the operation of a RabbitMQ
broker is replicated across all nodes. An exception to this
are message queues, which by default reside on one node,
though they are visible and reachable from all nodes. To
replicate queues across nodes in a cluster, use a queue type
that supports replication. This topic is covered in
the <a href="/quorum-queues.html">Quorum Queues</a> guide.</p>
<h3><a id="peer-equality" class="anchor" href="#peer-equality">Nodes are Equal Peers</a></h3>
<p>Some distributed systems
have leader and follower nodes. This is generally not true for RabbitMQ.
All nodes in a RabbitMQ cluster are equal peers: there are no special nodes in RabbitMQ core.
This topic becomes more nuanced when <a href="quorum-queues.html">quorum queues</a> and plugins
are taken into consideration but for most intents and purposes,
all cluster nodes should be considered equal.</p>
<p>Many <a href="/cli.html">CLI tool</a> operations can be executed against any node.
An <a href="/management.html">HTTP API</a> client can target any cluster node.</p>
<p>Individual plugins can designate (elect)
certain nodes to be "special" for a period of time. For example, <a href="/federation.html">federation links</a>
are colocated on a particular cluster node. Should that node fail, the links will
be restarted on a different node.</p>
<p>In versions older than 3.6.7, <a href="/management.html">RabbitMQ management plugin</a> used
a dedicated node for stats collection and aggregation.</p>
<h3><a id="erlang-cookie" class="anchor" href="#erlang-cookie">How CLI Tools Authenticate to Nodes (and Nodes to Each Other): the Erlang Cookie</a></h3>
<p>RabbitMQ nodes and CLI tools (e.g. <span class="code ">rabbitmqctl</span>) use a
cookie to determine whether they are allowed to communicate with
each other. For two nodes to be able to communicate they must have
the same shared secret called the Erlang cookie. The cookie is
just a string of alphanumeric characters up to 255 characters in size.
It is usually stored in a local file. The file must be only
accessible to the owner (e.g. have UNIX permissions of <span class="code ">600</span> or similar).
Every cluster node must have the same cookie.</p>
<p>If the file does not exist, Erlang VM will try to create
one with a randomly generated value when the RabbitMQ server
starts up. Using such generated cookie files are appropriate in development
environments only. Since each node will generate its own value independently,
this strategy is not really viable in a <a href="/clustering.html">clustered environment</a>.</p>
<p>Erlang cookie generation should be done at cluster deployment stage, ideally using automation
and orchestration tools.</p>
<p>In distributed deployment</p>
<h3><a id="cookie-file-locations" class="anchor" href="#cookie-file-locations">Cookie File Locations</a></h3>
<h4>Linux, MacOS, *BSD</h4>
<p>On UNIX systems, the cookie will be typically
located in <span class="code ">/var/lib/rabbitmq/.erlang.cookie</span> (used by the server)
and <span class="code ">$HOME/.erlang.cookie</span> (used by CLI tools). Note that since the value
of <span class="code ">$HOME</span> varies from user to user, it's necessary to place a copy of
the cookie file for each user that will be using the CLI tools.
This applies to both non-privileged users and <span class="code ">root</span>.</p>
<p>RabbitMQ nodes will log its effective user's home directory location early on boot.</p>
<h4>Community Docker Image and Kubernetes</h4>
<p><a href="https://github.com/docker-library/rabbitmq/">Docker community RabbitMQ image</a> uses <span class="code ">RABBITMQ_ERLANG_COOKIE</span> environment variable value
to populate the cookie file.</p>
<p>Configuration management and container orchestration tools that use this image
must make sure that every RabbitMQ node container in a cluster uses the same value.</p>
<p>In the context of Kubernetes, the value must be specified in the pod template specification of
the <a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">stateful set</a>.
For instance, this can be seen in the <a href="https://github.com/rabbitmq/diy-kubernetes-examples">RabbitMQ on Kubernetes examples repository</a>.</p>
<h4>Windows</h4>
<p>On Windows, the cookie location depends on a few factors:</p>
<ul>
<li>Erlang version: prior to 20.2 or 20.2 and later</li>
<li>Whether the <span class="code ">HOMEDRIVE</span> and <span class="code ">HOMEPATH</span> environment variables are both set</li>
</ul>
<h5>Erlang 20.2 or later</h5>
<p>With Erlang versions starting with 20.2, the cookie file locations are:</p>
<ul>
<li><span class="code ">%HOMEDRIVE%%HOMEPATH%\.erlang.cookie</span> (usually <span class="code ">C:\Users\%USERNAME%\.erlang.cookie</span> for user <span class="code ">%USERNAME%</span>) if both the <span class="code ">HOMEDRIVE</span> and <span class="code ">HOMEPATH</span> environment variables are set</li>
<li><span class="code ">%USERPROFILE%\.erlang.cookie</span> (usually <span class="code ">C:\Users\%USERNAME%\.erlang.cookie</span>) if <span class="code ">HOMEDRIVE</span> and <span class="code ">HOMEPATH</span> are not both set</li>
<li>For the RabbitMQ Windows service - <span class="code ">%USERPROFILE%\.erlang.cookie</span> (usually <span class="code ">C:\WINDOWS\system32\config\systemprofile</span>)</li>
</ul>
<p>If the Windows service is used, the cookie should be copied from
<span class="code ">C:\Windows\system32\config\systemprofile\.erlang.cookie</span> to the expected
location for users running commands like <span class="code ">rabbitmqctl.bat</span>.</p>
<h5>Erlang 19.3 through 20.2</h5>
<p>With Erlang versions prior to 20.2, the cookie file locations are:</p>
<ul>
<li><span class="code ">%HOMEDRIVE%%HOMEPATH%\.erlang.cookie</span> (usually <span class="code ">C:\Users\%USERNAME%\.erlang.cookie</span> for user <span class="code ">%USERNAME%</span>) if both the <span class="code ">HOMEDRIVE</span> and <span class="code ">HOMEPATH</span> environment variables are set</li>
<li><span class="code ">%USERPROFILE%\.erlang.cookie</span> (usually <span class="code ">C:\Users\%USERNAME%\.erlang.cookie</span>) if <span class="code ">HOMEDRIVE</span> and <span class="code ">HOMEPATH</span> are not both set</li>
<li>For the RabbitMQ Windows service - <span class="code ">%WINDIR%\.erlang.cookie</span> (usually <span class="code ">C:\Windows\.erlang.cookie</span>)</li>
</ul>
<p>If the Windows service is used, the cookie should be copied from
<span class="code ">C:\Windows\.erlang.cookie</span> to the expected location for users
running commands like <span class="code ">rabbitmqctl.bat</span>.</p>
<h3>Overriding Using CLI and Runtime Command Line Arguments</h3>
<p>As an alternative, the option "<span class="code ">-setcookie &lt;value&gt;</span>" can be added
to <span class="code ">RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS</span> <a href="/configure.html">environment variable value</a>
to override the cookie value used by a RabbitMQ node:</p>
<pre class="lang-bash">
RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS="-setcookie cookie-value"
</pre>

<p>CLI tools can take a cookie value using a command line flag:</p>
<pre class="lang-bash">
rabbitmq-diagnostics status --erlang-cookie "cookie-value"
</pre>

<p>Both are <strong>the least secure options</strong> and generally <strong>not recommended</strong>.</p>
<h3>Troubleshooting</h3>
<p>When a node starts, it will <a href="/logging.html">log</a> the home directory location of its effective user:</p>
<pre class="lang-plaintext">
node           : rabbit@cdbf4de5f22d
home dir       : /var/lib/rabbitmq
</pre>

<p>Unless any <a href="/relocate.html">server directories</a> were overridden, that's the directory where
the cookie file will be looked for, and created by the node on first boot if it does not already exist.</p>
<p>In the example above, the cookie file location will be <span class="code ">/var/lib/rabbitmq/.erlang.cookie</span>.</p>
<h3><a id="cli-authentication-failures" class="anchor" href="#cli-authentication-failures">Authentication Failures</a></h3>
<p>When the cookie is misconfigured (for example, not identical), RabbitMQ nodes will log errors
such as "Connection attempt from disallowed node", "", "Could not auto-cluster".</p>
<p>For example, when a CLI tool connects and tries to authenticate using a mismatching secret value:</p>
<pre class="lang-plaintext">
2020-06-15 13:03:33 [error] &lt;0.1187.0&gt; ** Connection attempt from node 'rabbitmqcli-99391-rabbit@warp10' rejected. Invalid challenge reply. **
</pre>

<p>When a CLI tool such as <span class="code ">rabbitmqctl</span> fails to authenticate with RabbitMQ,
the message usually says</p>
<pre class="lang-plaintext">
* epmd reports node 'rabbit' running on port 25672
* TCP connection succeeded but Erlang distribution failed
* suggestion: hostname mismatch?
* suggestion: is the cookie set correctly?
* suggestion: is the Erlang distribution using TLS?
</pre>

<p>An incorrectly placed cookie file or cookie value mismatch are most common scenarios for such failures.</p>
<p>When a recent Erlang/OTP version is used, authentication failures contain
more information and cookie mismatches can be identified better:</p>
<pre class="lang-ini">
* connected to epmd (port 4369) on warp10
* epmd reports node 'rabbit' running on port 25672
* TCP connection succeeded but Erlang distribution failed

* Authentication failed (rejected by the remote node), please check the Erlang cookie
</pre>

<p>See the <a href="/cli.html">CLI Tools guide</a> for more information.</p>
<h3><a id="cookie-file-troubleshooting" class="anchor" href="#cookie-file-troubleshooting">Troubleshooting</a> Cookie-based Authentication</h3>
<h4>Server Nodes</h4>
<p>When a node starts, it will <a href="/logging.html">log</a> the home directory location of its effective user:</p>
<pre class="lang-plaintext">
node           : rabbit@cdbf4de5f22d
home dir       : /var/lib/rabbitmq
</pre>

<p>Unless any <a href="/relocate.html">server directories</a> were overridden, that's the directory where
the cookie file will be looked for, and created by the node on first boot if it does not already exist.</p>
<p>In the example above, the cookie file location will be <span class="code ">/var/lib/rabbitmq/.erlang.cookie</span>.</p>
<h4>Hostname Resolution</h4>
<p>Since hostname resolution is a <a href="#hostname-resolution-requirement">prerequisite for successful inter-node communication</a>,
starting with <a href="/changelog.html">RabbitMQ <span class="code ">3.8.6</span></a>, CLI tools provide two commands that help verify
that hostname resolution on a node works as expected. The commands are not meant to replace
<a href="https://en.wikipedia.org/wiki/Dig_(command)"><span class="code ">dig</span></a> and other specialised DNS tools but rather
provide a way to perform most basic checks while taking <a href="https://erlang.org/doc/apps/erts/inet_cfg.html">Erlang runtime hostname resolver features</a>
into account.</p>
<p>The commands are covered in the <a href="/networking.html#dns-verify-resolution">Networking guide</a>.</p>
<h4>CLI Tools</h4>
<p>Starting with <a href="/changelog.html">version <span class="code ">3.8.6</span></a>, <span class="code ">rabbitmq-diagnostics</span> includes a command
that provides relevant information on the Erlang cookie file used by CLI tools:</p>
<pre class="lang-bash">
rabbitmq-diagnostics erlang_cookie_sources
</pre>

<p>The command will report on the effective user, user home directory and the expected location
of the cookie file:</p>
<pre class="lang-plaintext">
Cookie File

Effective user: antares
Effective home directory: /home/cli-user
Cookie file path: /home/cli-user/.erlang.cookie
Cookie file exists? true
Cookie file type: regular
Cookie file access: read
Cookie file size: 20

Cookie CLI Switch

--erlang-cookie value set? false
--erlang-cookie value length: 0

Env variable  (Deprecated)

RABBITMQ_ERLANG_COOKIE value set? false
RABBITMQ_ERLANG_COOKIE value length: 0
</pre>

<h2><a id="node-count" class="anchor" href="#node-count">Node Counts and Quorum</a></h2>
<p>Because several features (e.g. <a href="/quorum-queues.html">quorum queues</a>, <a href="/mqtt.html">client tracking in MQTT</a>)
require a consensus between cluster members, odd numbers of cluster nodes are highly recommended:
1, 3, 5, 7 and so on.</p>
<p>Two node clusters are <strong>highly recommended against</strong> since it's impossible for cluster nodes to identify
a majority and form a consensus in case of connectivity loss. For example, when the two nodes lose connectivity
MQTT client connections won't be accepted, quorum queues would lose their availability, and so on.</p>
<p>From the consensus point of view, four or six node clusters would have the same availability
characteristics as three and five node clusters.</p>
<p>The <a href="/quorum-queues.html">Quorum Queues guide</a> covers this topic in more detail.</p>
<h2><a id="clustering-and-clients" class="anchor" href="#clustering-and-clients">Clustering and Clients</a></h2>
<p>Assuming all cluster members
are available, a client can connect to any node and
perform any operation. Nodes will route operations to the
<a href="/quorum-queues.html">quorum queue leader</a> or <a href="ha.html#master-migration-data-locality">queue master replica</a>
transparently to clients.</p>
<p>With all supported messaging protocols a client is only connected to one node
at a time.</p>
<p>In case of a node failure, clients should be able to reconnect
to a different node, recover their topology and continue operation. For
this reason, most client libraries accept a list of endpoints (hostnames or IP addresses)
as a connection option. The list of hosts will be used during initial connection
as well as connection recovery, if the client supports it. See documentation guides
for individual clients to learn more.</p>
<p>With <a href="/quorum-queues.html">quorum queues</a>, clients will only be able to perform
operations on queues that have a quorum of replicas online.</p>
<p>With classic mirrored queues, there are scenarios where it may not be possible for a client to transparently continue
operations after connecting to a different node. They usually involve
<a href="/ha.html#non-mirrored-queue-behavior-on-node-failure">non-mirrored queues hosted on a failed node</a>.</p>
<h2><a id="clustering-and-observability" class="anchor" href="#clustering-and-observability">Clustering and Observability</a></h2>
<p>Client connections, channels and queues will be distributed across cluster nodes.
Operators need to be able to inspect and <a href="/monitoring.html">monitor</a> such resources
across all cluster nodes.</p>
<p>RabbitMQ <a href="/cli.html">CLI tools</a> such as <span class="code ">rabbitmq-diagnostics</span> and <span class="code ">rabbitmqctl</span>
provide commands that inspect resources and cluster-wide state. Some commands focus on the state of a single node
(e.g. <span class="code ">rabbitmq-diagnostics environment</span> and <span class="code ">rabbitmq-diagnostics status</span>), others
inspect cluster-wide state. Some examples of the latter include <span class="code ">rabbitmqctl list_connections</span>,
<span class="code ">rabbitmqctl list_mqtt_connections</span>, <span class="code ">rabbitmqctl list_stomp_connections</span>, <span class="code ">rabbitmqctl list_users</span>,
<span class="code ">rabbitmqctl list_vhosts</span> and so on.</p>
<p>Such "cluster-wide" commands will often contact one node
first, discover cluster members and contact them all to
retrieve and combine their respective state. For example,
<span class="code ">rabbitmqctl list_connections</span> will contact all
nodes, retrieve their AMQP 0-9-1 and AMQP 1.0 connections,
and display them all to the user.  The user doesn't have
to manually contact all nodes. Assuming a non-changing
state of the cluster (e.g. no connections are closed or
opened), two CLI commands executed against two different
nodes one after another will produce identical or
semantically identical results. "Node-local" commands, however, will not produce
identical results since two nodes rarely have identical state: at the very least their
node names will be different!</p>
<p><a href="/management.html">Management UI</a> works similarly: a node that has to respond to an HTTP API request
will fan out to other cluster members and aggregate their responses. In a cluster with multiple nodes that have management plugin
enabled, the operator can use any node to access management UI. The same goes for monitoring tools that use
the HTTP API to collect data about the state of the cluster. There is no need to issue a request to every cluster node in turn.</p>
<h3><a id="clustering-dealing-with-failure" class="anchor" href="#clustering-dealing-with-failure">Node Failure Handling</a></h3>
<p>RabbitMQ brokers tolerate the failure of individual
nodes. Nodes can be started and stopped at will,
as long as they can contact a cluster member node
known at the time of shutdown.</p>
<p><a href="quorum-queues.html">Quorum queue</a> allows queue contents to be replicated
across multiple cluster nodes with parallel replication and a predictable <a href="#quorum-queues.html#leader-election">leader election</a>
and <a href="quorum-queues.html#data-safety">data safety</a> behavior as long as a majority of replicas are online.</p>
<p>Non-replicated classic queues can also be used in clusters. Non-mirrored queue <a href="/ha.html#non-mirrored-queue-behavior-on-node-failure">behaviour in case of node failure</a>
depends on <a href="queues.html#durability">queue durability</a>.</p>
<p>RabbitMQ clustering has several modes of dealing with <a href="partitions.html">network partitions</a>,
primarily consistency oriented. Clustering is meant to be used across LAN. It is
not recommended to run clusters that span WAN.
The <a href="shovel.html">Shovel</a> or
<a href="federation.html">Federation</a>
plugins are better solutions for connecting brokers across a
WAN. Note that <a href="distributed.html">Shovel and Federation are not equivalent to clustering</a>.</p>
<h3><a id="clustering-and-stats" class="anchor" href="#clustering-and-stats">Metrics and Statistics</a></h3>
<p>Every node stores and aggregates its own metrics and stats, and provides an API for
other nodes to access it. Some stats are cluster-wide, others are specific to individual nodes.
Node that responds to an <a href="/management.html">HTTP API</a> request contacts its peers
to retrieve their data and then produces an aggregated result.</p>
<p>In versions older than 3.6.7, <a href="/management.html">RabbitMQ management plugin</a> used
a dedicated node for stats collection and aggregation.</p>
<h2><a id="manual-transcript" class="anchor" href="#manual-transcript">Clustering Transcript with <span class="code ">rabbitmqctl</span></a></h2>
<p>The following several sections provide a transcript of manually setting up and manipulating
a RabbitMQ cluster across three machines: <span class="code ">rabbit1</span>, <span class="code ">rabbit2</span>,
<span class="code ">rabbit3</span>. It is recommended that the example is studied before
<a href="/cluster-formation.html">more automation-friendly</a> cluster formation
options are used.</p>
<p>We assume that the user is logged into all three machines,
that RabbitMQ has been installed on the machines, and that
the rabbitmq-server and rabbitmqctl scripts are in the
user's PATH.</p>
<p>This transcript can be modified to run on a single host, as
explained more details below.</p>
<h2><a id="starting" class="anchor" href="#starting">Starting Independent Nodes</a></h2>
<p>Clusters are set up by re-configuring existing RabbitMQ
nodes into a cluster configuration. Hence the first step
is to start RabbitMQ on all nodes in the normal way:</p>
<pre class="lang-bash">
# on rabbit1
rabbitmq-server -detached
# on rabbit2
rabbitmq-server -detached
# on rabbit3
rabbitmq-server -detached
</pre>

<p>This creates three <i>independent</i> RabbitMQ brokers,
one on each node, as confirmed by the <i>cluster_status</i>
command:</p>
<pre class="lang-bash">
# on rabbit1
rabbitmqctl cluster_status
# =&gt; Cluster status of node rabbit@rabbit1 ...
# =&gt; [{nodes,[{disc,[rabbit@rabbit1]}]},{running_nodes,[rabbit@rabbit1]}]
# =&gt; ...done.

# on rabbit2
rabbitmqctl cluster_status
# =&gt; Cluster status of node rabbit@rabbit2 ...
# =&gt; [{nodes,[{disc,[rabbit@rabbit2]}]},{running_nodes,[rabbit@rabbit2]}]
# =&gt; ...done.

# on rabbit3
rabbitmqctl cluster_status
# =&gt; Cluster status of node rabbit@rabbit3 ...
# =&gt; [{nodes,[{disc,[rabbit@rabbit3]}]},{running_nodes,[rabbit@rabbit3]}]
# =&gt; ...done.
</pre>

<p>The node name of a RabbitMQ broker started from the
<span class="code ">rabbitmq-server</span> shell script is
<span class="code ">rabbit@<i>shorthostname</i></span>, where the short
node name is lower-case (as in <span class="code ">rabbit@rabbit1</span>,
above). On Windows, if <span class="code ">rabbitmq-server.bat</span>
batch file is used, the short node name is upper-case (as
in <span class="code ">rabbit@RABBIT1</span>). When you type node names,
case matters, and these strings must match exactly.</p>
<h2><a id="creating" class="anchor" href="#creating">Creating a Cluster</a></h2>
<p>In order to link up our three nodes in a cluster, we tell
two of the nodes, say <span class="code ">rabbit@rabbit2</span> and
<span class="code ">rabbit@rabbit3</span>, to join the cluster of the
third, say <span class="code ">rabbit@rabbit1</span>. Prior to that both
newly joining members must be <a href="/rabbitmqctl.8.html#reset">reset</a>.</p>
<p>We first join <span class="code ">rabbit@rabbit2</span> in a cluster
with <span class="code ">rabbit@rabbit1</span>. To do that, on
<span class="code ">rabbit@rabbit2</span> we stop the RabbitMQ
application and join the <span class="code ">rabbit@rabbit1</span>
cluster, then restart the RabbitMQ application. Note that
a node must be <a href="/rabbitmqctl.8.html#reset">reset</a> before it can join an existing cluster.
Resetting the node <strong>removes all resources and data that were previously
present on that node</strong>. This means that a node cannot be made a member
of a cluster and keep its existing data at the same time. When that's desired,
using the <a href="/blue-green-upgrade.html">Blue/Green deployment strategy</a> or <a href="/backup.html">backup and restore</a>
are the available options.</p>
<pre class="lang-bash">
# on rabbit2
rabbitmqctl stop_app
# =&gt; Stopping node rabbit@rabbit2 ...done.

rabbitmqctl reset
# =&gt; Resetting node rabbit@rabbit2 ...

rabbitmqctl join_cluster rabbit@rabbit1
# =&gt; Clustering node rabbit@rabbit2 with [rabbit@rabbit1] ...done.

rabbitmqctl start_app
# =&gt; Starting node rabbit@rabbit2 ...done.
</pre>

<p>We can see that the two nodes are joined in a cluster by
running the <i>cluster_status</i> command on either of the nodes:</p>
<pre class="lang-bash">
# on rabbit1
rabbitmqctl cluster_status
# =&gt; Cluster status of node rabbit@rabbit1 ...
# =&gt; [{nodes,[{disc,[rabbit@rabbit1,rabbit@rabbit2]}]},
# =&gt;  {running_nodes,[rabbit@rabbit2,rabbit@rabbit1]}]
# =&gt; ...done.

# on rabbit2
rabbitmqctl cluster_status
# =&gt; Cluster status of node rabbit@rabbit2 ...
# =&gt; [{nodes,[{disc,[rabbit@rabbit1,rabbit@rabbit2]}]},
# =&gt;  {running_nodes,[rabbit@rabbit1,rabbit@rabbit2]}]
# =&gt; ...done.
</pre>

<p>Now we join <span class="code ">rabbit@rabbit3</span> to the same
cluster. The steps are identical to the ones above, except
this time we'll cluster to <span class="code ">rabbit2</span> to
demonstrate that the node chosen to cluster to does not
matter - it is enough to provide one online node and the
node will be clustered to the cluster that the specified
node belongs to.</p>
<pre class="lang-bash">
# on rabbit3
rabbitmqctl stop_app
# =&gt; Stopping node rabbit@rabbit3 ...done.

# on rabbit3
rabbitmqctl reset
# =&gt; Resetting node rabbit@rabbit3 ...

rabbitmqctl join_cluster rabbit@rabbit2
# =&gt; Clustering node rabbit@rabbit3 with rabbit@rabbit2 ...done.

rabbitmqctl start_app
# =&gt; Starting node rabbit@rabbit3 ...done.
</pre>

<p>We can see that the three nodes are joined in a cluster by
running the <span class="code ">cluster_status</span> command on any of the nodes:</p>
<pre class="lang-bash">
# on rabbit1
rabbitmqctl cluster_status
# =&gt; Cluster status of node rabbit@rabbit1 ...
# =&gt; [{nodes,[{disc,[rabbit@rabbit1,rabbit@rabbit2,rabbit@rabbit3]}]},
# =&gt;  {running_nodes,[rabbit@rabbit3,rabbit@rabbit2,rabbit@rabbit1]}]
# =&gt; ...done.

# on rabbit2
rabbitmqctl cluster_status
# =&gt; Cluster status of node rabbit@rabbit2 ...
# =&gt; [{nodes,[{disc,[rabbit@rabbit1,rabbit@rabbit2,rabbit@rabbit3]}]},
# =&gt;  {running_nodes,[rabbit@rabbit3,rabbit@rabbit1,rabbit@rabbit2]}]
# =&gt; ...done.

# on rabbit3
rabbitmqctl cluster_status
# =&gt; Cluster status of node rabbit@rabbit3 ...
# =&gt; [{nodes,[{disc,[rabbit@rabbit3,rabbit@rabbit2,rabbit@rabbit1]}]},
# =&gt;  {running_nodes,[rabbit@rabbit2,rabbit@rabbit1,rabbit@rabbit3]}]
# =&gt; ...done.
</pre>

<p>By following the above steps we can add new nodes to the
cluster at any time, while the cluster is running.</p>
<h2><a id="restarting" class="anchor" href="#restarting">Restarting Cluster Nodes</a></h2>
<p>Nodes that have been joined to a cluster can be stopped at
any time. They can also fail or be terminated by the OS.</p>
<p>In general, if the majority of nodes is still online after a node
is stopped, this does not affect the rest of the cluster, although
client connection distribution, queue replica placement, and load distribution
of the cluster will change.</p>
<h3><a id="restarting-schema-sync" class="anchor" href="#restarting-schema-sync">Schema Syncing from Online Peers</a></h3>
<p>A restarted node will sync the schema
and other information from its peers on boot. Before this process
completes, the node <strong>won't be fully started and functional</strong>.</p>
<p>It is therefore important to understand the process node go through when
they are stopped and restarted.</p>
<p>A stopping node picks an online cluster member (only disc
nodes will be considered) to sync with after restart. Upon
restart the node will try to contact that peer 10 times by
default, with 30 second response timeouts.</p>
<p>In case the peer becomes available in that time interval, the node
successfully starts, syncs what it needs from the peer and
keeps going.</p>
<p>If the peer does not become available, the restarted
node will <strong>give up and voluntarily stop</strong>. Such condition can be
identified by the timeout (<span class="code ">timeout_waiting_for_tables</span>) warning messages in the logs
that eventually lead to node startup failure:</p>
<pre class="lang-plaintext">
2020-07-27 21:10:51.361 [warning] &lt;0.269.0&gt; Error while waiting for Mnesia tables: {timeout_waiting_for_tables,[rabbit@node2,rabbit@node1],[rabbit_durable_queue]}
2020-07-27 21:10:51.361 [info] &lt;0.269.0&gt; Waiting for Mnesia tables for 30000 ms, 1 retries left
2020-07-27 21:11:21.362 [warning] &lt;0.269.0&gt; Error while waiting for Mnesia tables: {timeout_waiting_for_tables,[rabbit@node2,rabbit@node1],[rabbit_durable_queue]}
2020-07-27 21:11:21.362 [info] &lt;0.269.0&gt; Waiting for Mnesia tables for 30000 ms, 0 retries left
</pre>

<pre class="lang-plaintext">
2020-07-27 21:15:51.380 [info] &lt;0.269.0&gt; Waiting for Mnesia tables for 30000 ms, 1 retries left
2020-07-27 21:16:21.381 [warning] &lt;0.269.0&gt; Error while waiting for Mnesia tables: {timeout_waiting_for_tables,[rabbit@node2,rabbit@node1],[rabbit_user,rabbit_user_permission, â€¦]}
2020-07-27 21:16:21.381 [info] &lt;0.269.0&gt; Waiting for Mnesia tables for 30000 ms, 0 retries left
2020-07-27 21:16:51.393 [info] &lt;0.44.0&gt; Application mnesia exited with reason: stopped
</pre>

<pre class="lang-plaintext">
2020-07-27 21:16:51.397 [error] &lt;0.269.0&gt; BOOT FAILED
2020-07-27 21:16:51.397 [error] &lt;0.269.0&gt; ===========
2020-07-27 21:16:51.397 [error] &lt;0.269.0&gt; Timeout contacting cluster nodes: [rabbit@node1].
</pre>

<p>When a node has no online peers during shutdown, it will start without
attempts to sync with any known peers. It does not start as a standalone
node, however, and peers will be able to rejoin it.</p>
<p>When the entire cluster is brought down therefore, the last node to go down
is the only one that didn't have any running peers at the time of shutdown.
That node can start without contacting any peers first.
Since nodes will try to contact a known peer for up to 5 minutes (by default), nodes
can be restarted in any order in that period of time. In this case
they will rejoin each other one by one successfully. This window of time
can be adjusted using two configuration settings:</p>
<pre class="lang-ini">
# wait for 60 seconds instead of 30
mnesia_table_loading_retry_timeout = 60000

# retry 15 times instead of 10
mnesia_table_loading_retry_limit = 15
</pre>

<p>By adjusting these settings and tweaking the time window in which
known peer has to come back it is possible to account for cluster-wide
redeployment scenarios that can be longer than 5 minutes to complete.</p>
<p>During <a href="/upgrade.html">upgrades</a>, sometimes the last node to stop
must be the first node to be started after the upgrade. That node will be designated to perform
a cluster-wide schema migration that other nodes can sync from and apply when they
rejoin.</p>
<h3><a id="restarting-readiness-probes" class="anchor" href="#restarting-readiness-probes">Restarts and Health Checks (Readiness Probes)</a></h3>
<p>In some environments, node restarts are controlled with a designated <a href="/monitoring.html#health-checks">health check</a>.
The checks verify that one node has started and the deployment process can proceed to the next one.
If the check does not pass, the deployment of the node is considered to be incomplete and the deployment process
will typically wait and retry for a period of time. One popular example of such environment is Kubernetes
where an operator-defined <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-readiness-gate">readiness probe</a>
can prevent a deployment from proceeding when the <a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#deployment-and-scaling-guarantees"><span class="code ">OrderedReady</span> pod management policy</a> is used. Deployments that use the <span class="code ">Parallel</span> pod management policy
will not be affected but must worry about the <a href="/cluster-formation.html#initial-formation-race-condition">natural race condition during initial cluster formation</a>.</p>
<p>Given the <a href="#restarting-schema-sync">peer syncing behavior described above</a>, such a health check can prevent a cluster-wide restart
from completing in time. Checks that explicitly or implicitly assume a fully booted node that's rejoined
its cluster peers will fail and block further node deployments.</p>
<p><a href="/monitoring.html#health-checks">Most health check</a>, even relatively basic ones, implicitly assume that the node has
finished booting. They are not suitable for nodes that are <a href="#restarting-schema-sync">awaiting schema table sync</a> from a peer.</p>
<p>One very common example of such check is</p>
<pre class="lang-bash">
# will exit with an error for the nodes that are currently waiting for
# a peer to sync schema tables from
rabbitmq-diagnostics check_running
</pre>

<p>One health check that does not expect a node to be fully booted and have schema tables synced is</p>
<pre class="lang-bash">
# a very basic check that will succeed for the nodes that are currently waiting for
# a peer to sync schema from
rabbitmq-diagnostics ping
</pre>

<p>This basic check would allow the deployment to proceed and the nodes to eventually rejoin each other,
assuming they are <a href="/upgrade.html">compatible</a>.</p>
<h3><a id="restarting-with-hostname-changes" class="anchor" href="#restarting-with-hostname-changes">Hostname Changes Between Restarts</a></h3>
<p>A node rejoining after a node name or host name change can start as <a href="/cluster-formation.html#peer-discovery-how-does-it-work">a blank node</a>
if its data directory path changes as a result. Such nodes will fail to rejoin the cluster.
While the node is offline, its peers can be reset or started with a blank data directory.
In that case the recovering node will fail to rejoin its peer as well since internal data store cluster
identity would no longer match.</p>
<p>Consider the following scenario:</p>
<ol>
<li>A cluster of 3 nodes, A, B and C is formed</li>
<li>Node A is shut down</li>
<li>Node B is reset</li>
<li>Node A is started</li>
<li>Node A tries to rejoin B but B's cluster identity has changed</li>
<li>Node B doesn't recognise A as a known cluster member because it's been reset</li>
</ol>
<p>in this case node B will reject the clustering attempt from A with an appropriate error
message in the log:</p>
<pre class="lang-plaintext">
Node 'rabbit@node1.local' thinks it's clustered with node 'rabbit@node2.local', but 'rabbit@node2.local' disagrees
</pre>

<p>In this case B can be reset again and then will be able to join A, or A
can be reset and will successfully join B.</p>
<h3><a id="restarting-transcript" class="anchor" href="#restarting-transcript">Cluster Node Restart Example</a></h3>
<p>The below example uses CLI tools to shut down the nodes <span class="code ">rabbit@rabbit1</span> and
<span class="code ">rabbit@rabbit3</span> and check on the cluster
status at each step:</p>
<pre class="lang-bash">
# on rabbit1
rabbitmqctl stop
# =&gt; Stopping and halting node rabbit@rabbit1 ...done.

# on rabbit2
rabbitmqctl cluster_status
# =&gt; Cluster status of node rabbit@rabbit2 ...
# =&gt; [{nodes,[{disc,[rabbit@rabbit1,rabbit@rabbit2,rabbit@rabbit3]}]},
# =&gt;  {running_nodes,[rabbit@rabbit3,rabbit@rabbit2]}]
# =&gt; ...done.

# on rabbit3
rabbitmqctl cluster_status
# =&gt; Cluster status of node rabbit@rabbit3 ...
# =&gt; [{nodes,[{disc,[rabbit@rabbit1,rabbit@rabbit2,rabbit@rabbit3]}]},
# =&gt;  {running_nodes,[rabbit@rabbit2,rabbit@rabbit3]}]
# =&gt; ...done.

# on rabbit3
rabbitmqctl stop
# =&gt; Stopping and halting node rabbit@rabbit3 ...done.

# on rabbit2
rabbitmqctl cluster_status
# =&gt; Cluster status of node rabbit@rabbit2 ...
# =&gt; [{nodes,[{disc,[rabbit@rabbit1,rabbit@rabbit2,rabbit@rabbit3]}]},
# =&gt;  {running_nodes,[rabbit@rabbit2]}]
# =&gt; ...done.
</pre>

<p>In the below example, the nodes are started back, checking on the cluster
status as we go along:</p>
<pre class="lang-bash">
# on rabbit1
rabbitmq-server -detached
rabbitmqctl cluster_status
# =&gt; Cluster status of node rabbit@rabbit1 ...
# =&gt; [{nodes,[{disc,[rabbit@rabbit1,rabbit@rabbit2,rabbit@rabbit3]}]},
# =&gt;  {running_nodes,[rabbit@rabbit2,rabbit@rabbit1]}]
# =&gt; ...done.

# on rabbit2
rabbitmqctl cluster_status
# =&gt; Cluster status of node rabbit@rabbit2 ...
# =&gt; [{nodes,[{disc,[rabbit@rabbit1,rabbit@rabbit2,rabbit@rabbit3]}]},
# =&gt;  {running_nodes,[rabbit@rabbit1,rabbit@rabbit2]}]
# =&gt; ...done.

# on rabbit3
rabbitmq-server -detached

# on rabbit1
rabbitmqctl cluster_status
# =&gt; Cluster status of node rabbit@rabbit1 ...
# =&gt; [{nodes,[{disc,[rabbit@rabbit1,rabbit@rabbit2,rabbit@rabbit3]}]},
# =&gt;  {running_nodes,[rabbit@rabbit2,rabbit@rabbit1,rabbit@rabbit3]}]
# =&gt; ...done.

# on rabbit2
rabbitmqctl cluster_status
# =&gt; Cluster status of node rabbit@rabbit2 ...
# =&gt; [{nodes,[{disc,[rabbit@rabbit1,rabbit@rabbit2,rabbit@rabbit3]}]},
# =&gt;  {running_nodes,[rabbit@rabbit1,rabbit@rabbit2,rabbit@rabbit3]}]
# =&gt; ...done.

# on rabbit3
rabbitmqctl cluster_status
# =&gt; Cluster status of node rabbit@rabbit3 ...
# =&gt; [{nodes,[{disc,[rabbit@rabbit1,rabbit@rabbit2,rabbit@rabbit3]}]},
# =&gt;  {running_nodes,[rabbit@rabbit2,rabbit@rabbit1,rabbit@rabbit3]}]
# =&gt; ...done.
</pre>

<h2><a id="forced-boot" class="anchor" href="#forced-boot">Forcing Node Boot in Case of Unavailable Peers</a></h2>
<p>In some cases the last node to go
offline cannot be brought back up. It can be removed from the
cluster using the <span class="code ">forget_cluster_node</span> <a href="/cli.html">rabbitmqctl</a> command.</p>
<p>Alternatively <span class="code ">force_boot</span> <a href="/cli.html">rabbitmqctl</a> command can be used
on a node to make it boot without trying to sync with any
peers (as if they were last to shut down). This is
usually only necessary if the last node to shut down or a
set of nodes will never be brought back online.</p>
<h2><a id="removing-nodes" class="anchor" href="#removing-nodes">Breaking Up a Cluster</a></h2>
<p>Sometimes it is necessary to remove a node from a
cluster. The operator has to do this explicitly using a
<span class="code ">rabbitmqctl</span> command.</p>
<p>Some <a href="/cluster-formation.html">peer discovery mechanisms</a>
support node health checks and forced
removal of nodes not known to the discovery backend. That feature is
opt-in (disabled by default).</p>
<p>We first remove <span class="code ">rabbit@rabbit3</span> from the cluster, returning it to
independent operation. To do that, on <span class="code ">rabbit@rabbit3</span> we
stop the RabbitMQ application, reset the node, and restart the
RabbitMQ application.</p>
<pre class="lang-bash">
# on rabbit3
rabbitmqctl stop_app
# =&gt; Stopping node rabbit@rabbit3 ...done.

rabbitmqctl reset
# =&gt; Resetting node rabbit@rabbit3 ...done.
rabbitmqctl start_app
# =&gt; Starting node rabbit@rabbit3 ...done.
</pre>

<p>Note that it would have been equally valid to list
<span class="code ">rabbit@rabbit3</span> as a node.</p>
<p>Running the <i>cluster_status</i> command on the nodes confirms
that <span class="code ">rabbit@rabbit3</span> now is no longer part of
the cluster and operates independently:</p>
<pre class="lang-bash">
# on rabbit1
rabbitmqctl cluster_status
# =&gt; Cluster status of node rabbit@rabbit1 ...
# =&gt; [{nodes,[{disc,[rabbit@rabbit1,rabbit@rabbit2]}]},
# =&gt; {running_nodes,[rabbit@rabbit2,rabbit@rabbit1]}]
# =&gt; ...done.

# on rabbit2
rabbitmqctl cluster_status
# =&gt; Cluster status of node rabbit@rabbit2 ...
# =&gt; [{nodes,[{disc,[rabbit@rabbit1,rabbit@rabbit2]}]},
# =&gt;  {running_nodes,[rabbit@rabbit1,rabbit@rabbit2]}]
# =&gt; ...done.

# on rabbit3
rabbitmqctl cluster_status
# =&gt; Cluster status of node rabbit@rabbit3 ...
# =&gt; [{nodes,[{disc,[rabbit@rabbit3]}]},{running_nodes,[rabbit@rabbit3]}]
# =&gt; ...done.
</pre>

<p>We can also remove nodes remotely. This is useful, for example, when
having to deal with an unresponsive node. We can for example remove
<span class="code ">rabbit@rabbit1</span> from <span class="code ">rabbit@rabbit2</span>.</p>
<pre class="lang-bash">
# on rabbit1
rabbitmqctl stop_app
# =&gt; Stopping node rabbit@rabbit1 ...done.

# on rabbit2
rabbitmqctl forget_cluster_node rabbit@rabbit1
# =&gt; Removing node rabbit@rabbit1 from cluster ...
# =&gt; ...done.
</pre>

<p>Note that <span class="code ">rabbit1</span> still thinks it's clustered with
<span class="code ">rabbit2</span>, and trying to start it will result in an
error. We will need to reset it to be able to start it again.</p>
<pre class="lang-bash">
# on rabbit1
rabbitmqctl start_app
# =&gt; Starting node rabbit@rabbit1 ...
# =&gt; Error: inconsistent_cluster: Node rabbit@rabbit1 thinks it's clustered with node rabbit@rabbit2, but rabbit@rabbit2 disagrees

rabbitmqctl reset
# =&gt; Resetting node rabbit@rabbit1 ...done.

rabbitmqctl start_app
# =&gt; Starting node rabbit@rabbit1 ...
# =&gt; ...done.
</pre>

<p>The <span class="code ">cluster_status</span> command now shows all three nodes
operating as independent RabbitMQ brokers:</p>
<pre class="lang-bash">
# on rabbit1
rabbitmqctl cluster_status
# =&gt; Cluster status of node rabbit@rabbit1 ...
# =&gt; [{nodes,[{disc,[rabbit@rabbit1]}]},{running_nodes,[rabbit@rabbit1]}]
# =&gt; ...done.

# on rabbit2
rabbitmqctl cluster_status
# =&gt; Cluster status of node rabbit@rabbit2 ...
# =&gt; [{nodes,[{disc,[rabbit@rabbit2]}]},{running_nodes,[rabbit@rabbit2]}]
# =&gt; ...done.

# on rabbit3
rabbitmqctl cluster_status
# =&gt; Cluster status of node rabbit@rabbit3 ...
# =&gt; [{nodes,[{disc,[rabbit@rabbit3]}]},{running_nodes,[rabbit@rabbit3]}]
# =&gt; ...done.
</pre>

<p>Note that <span class="code ">rabbit@rabbit2</span> retains the residual
state of the cluster, whereas <span class="code ">rabbit@rabbit1</span>
and <span class="code ">rabbit@rabbit3</span> are freshly initialised
RabbitMQ brokers. If we want to re-initialise
<span class="code ">rabbit@rabbit2</span> we follow the same steps as
for the other nodes:</p>
<pre class="lang-bash">
# on rabbit2
rabbitmqctl stop_app
# =&gt; Stopping node rabbit@rabbit2 ...done.
rabbitmqctl reset
# =&gt; Resetting node rabbit@rabbit2 ...done.
rabbitmqctl start_app
# =&gt; Starting node rabbit@rabbit2 ...done.
</pre>

<p>Besides <span class="code ">rabbitmqctl forget_cluster_node</span> and the automatic cleanup of unknown nodes
by some <a href="/cluster-formation.html">peer discovery</a> plugins, there are no scenarios
in which a RabbitMQ node will permanently remove its peer node from a cluster.</p>
<h3><a id="resetting-nodes" class="anchor" href="#resetting-nodes">How to Reset a Node</a></h3>
<p>Sometimes it may be necessary to reset a node (wipe all of its data) and later make it rejoin the cluster.
Generally speaking, there are two possible scenarios: when the node is running, and when the node cannot start
or won't respond to CLI tool commands e.g. due to an issue such as <a href="https://bugs.erlang.org/browse/ERL-430">ERL-430</a>.</p>
<p>Resetting a node will delete all of its data, cluster membership information, configured <a href="/parameters.html">runtime parameters</a>,
users, virtual hosts and any other node data. It will also permanently remove the node from its cluster.</p>
<p>To reset a running and responsive node, first stop RabbitMQ on it using <span class="code ">rabbitmqctl stop_app</span>
and then reset it using <span class="code ">rabbitmqctl reset</span>:</p>
<pre class="lang-bash">
# on rabbit1
rabbitmqctl stop_app
# =&gt; Stopping node rabbit@rabbit1 ...done.
rabbitmqctl reset
# =&gt; Resetting node rabbit@rabbit1 ...done.
</pre>

<p>In case of a non-responsive node, it must be stopped first using any means necessary.
For nodes that fail to start this is already the case. Then <a href="/relocate.html">override</a>
the node's data directory location or [re]move the existing data store. This will make the node
start as a blank one. It will have to be instructed to <a href="#cluster-formation">rejoin its original cluster</a>, if any.</p>
<p>A node that's been reset and rejoined its original cluster will sync all virtual hosts, users, permissions
and topology (queues, exchanges, bindings), runtime parameters and policies. <a href="quorum-queues.html">Quorum queue</a>
contents will be replicated if the node will be selected to host a replica.
Non-replicated queue contents on a reset node will be lost.</p>
<h2><a id="upgrading" class="anchor" href="#upgrading">Upgrading clusters</a></h2>
<p>You can find instructions for upgrading a cluster in
<a href="/upgrade.html#rabbitmq-cluster-configuration">the upgrade guide</a>.</p>
<h2><a id="single-machine" class="anchor" href="#single-machine">A Cluster on a Single Machine</a></h2>
<p>Under some circumstances it can be useful to run a cluster
of RabbitMQ nodes on a single machine. This would
typically be useful for experimenting with clustering on a
desktop or laptop without the overhead of starting several
virtual machines for the cluster.</p>
<p>In order to run multiple RabbitMQ nodes on a single
machine, it is necessary to make sure the nodes have
distinct node names, data store locations, log file
locations, and bind to different ports, including those
used by plugins. See <span class="code ">RABBITMQ_NODENAME</span>,
<span class="code ">RABBITMQ_NODE_PORT</span>, and
<span class="code ">RABBITMQ_DIST_PORT</span> in the <a href="/configure.html#supported-environment-variables">Configuration
guide</a>, as well as <span class="code ">RABBITMQ_MNESIA_DIR</span>,
<span class="code ">RABBITMQ_CONFIG_FILE</span>, and
<span class="code ">RABBITMQ_LOG_BASE</span> in the <a href="/relocate.html">File and Directory Locations guide</a>.</p>
<p>You can start multiple nodes on the same host manually by
repeated invocation of <span class="code ">rabbitmq-server</span> (
<span class="code ">rabbitmq-server.bat</span> on Windows). For example:</p>
<pre class="lang-bash">
RABBITMQ_NODE_PORT=5672 RABBITMQ_NODENAME=rabbit rabbitmq-server -detached
RABBITMQ_NODE_PORT=5673 RABBITMQ_NODENAME=hare rabbitmq-server -detached
rabbitmqctl -n hare stop_app
rabbitmqctl -n hare join_cluster rabbit@`hostname -s`
rabbitmqctl -n hare start_app
</pre>

<p>will set up a two node cluster, both nodes as disc nodes.
Note that if the node <a href="/networking.html">listens on any ports</a> other
than AMQP 0-9-1 and AMQP 1.0 ones, those must be configured to avoid a collision as
well. This can be done via command line:</p>
<pre class="lang-bash">
RABBITMQ_NODE_PORT=5672 RABBITMQ_SERVER_START_ARGS="-rabbitmq_management listener [{port,15672}]" RABBITMQ_NODENAME=rabbit rabbitmq-server -detached
RABBITMQ_NODE_PORT=5673 RABBITMQ_SERVER_START_ARGS="-rabbitmq_management listener [{port,15673}]" RABBITMQ_NODENAME=hare rabbitmq-server -detached
</pre>

<p>will start two nodes (which can then be clustered) when
the management plugin is installed.</p>
<h2><a id="issues-hostname" class="anchor" href="#issues-hostname">Hostname Changes</a></h2>
<p>RabbitMQ nodes use hostnames to communicate with each other. Therefore,
all node names must be able to resolve names of all cluster peers. This is
also true for tools such as <span class="code ">rabbitmqctl</span>.</p>
<p>In addition to that, by default RabbitMQ names the database directory using the
current hostname of the system. If the hostname
changes, a new empty database is created. To avoid data loss it's
crucial to set up a fixed and resolvable hostname.</p>
<p>Whenever the hostname changes RabbitMQ node must be restarted.</p>
<p>A similar effect can be achieved by using <span class="code ">rabbit@localhost</span>
as the broker nodename.
The impact of this solution is that clustering will not work, because
the chosen hostname will not resolve to a routable address from remote
hosts. The <span class="code ">rabbitmqctl</span> command will similarly fail when
invoked from a remote host. A more sophisticated solution that does not
suffer from this weakness is to use DNS, e.g.
<a href="http://aws.amazon.com/route53/">Amazon Route 53</a> if running
on EC2. If you want to use the full hostname for your nodename (RabbitMQ
defaults to the short name), and that full hostname is resolvable using DNS,
you may want to investigate setting the environment variable
<span class="code ">RABBITMQ_USE_LONGNAME=true</span>.</p>
<p>See the section on <a href="/clustering.html#hostname-resolution-requirement">hostname resolution</a> for more information.</p>
<h2><a id="firewall" class="anchor" href="#firewall">Firewalled Nodes</a></h2>
<p>Nodes can have a firewall enabled on them. In such case, traffic on certain ports must be
allowed by the firewall in both directions, or nodes won't be able to join each other and
perform all the operations they expect to be available on cluster peers.</p>
<p>Learn more in the <a href="#ports">section on ports</a> above and dedicated <a href="/networking.html">RabbitMQ Networking guide</a>.</p>
<h2><a id="erlang" class="anchor" href="#erlang">Erlang Versions Across the Cluster</a></h2>
<p>All nodes in a cluster are <em>highly recommended</em> to run the same major <a href="/which-erlang.html">version of Erlang</a>: <span class="code ">22.2.0</span>
and <span class="code ">22.2.8</span> can be mixed but <span class="code ">21.3.6</span> and <span class="code ">22.2.6</span> can potentially introduce breaking changes in
inter-node communication protocols. While such breaking changes are relatively rare, they are possible.</p>
<p>Incompatibilities between patch releases of Erlang/OTP versions
are very rare.</p>
<h2><a id="clients" class="anchor" href="#clients">Connecting to Clusters from Clients</a></h2>
<p>A client can connect as normal to any node within a
cluster. If that node should fail, and the rest of the
cluster survives, then the client should notice the closed
connection, and should be able to reconnect to some
surviving member of the cluster.</p>
<p>Many clients support lists of hostnames that will be tried in order
at connection time.</p>
<p>Generally it is not recommended to hardcode IP addresses into
client applications: this introduces inflexibility and will
require client applications to be edited, recompiled and
redeployed should the configuration of the cluster change or
the number of nodes in the cluster change.</p>
<p>Instead, consider a more abstracted approach: this could be a
dynamic DNS service which has a very short TTL
configuration, or a plain TCP load balancer, or a combination of them.</p>
<p>In general, this aspect of managing the
connection to nodes within a cluster is beyond the scope of
this guide, and we recommend the use of other
technologies designed specifically to address these problems.</p>
<h2><a id="cluster-node-types" class="anchor" href="#cluster-node-types">Disk and RAM Nodes</a></h2>
<p>A node can be a <em>disk node</em> or a <em>RAM node</em>.
(<b>Note:</b> <i>disk</i> and <i>disc</i> are used
interchangeably). RAM nodes store internal database tables
in RAM only. This does not include messages, message store
indices, queue indices and other node state.</p>
<p>In the vast majority of cases you want all your nodes to be
disk nodes; RAM nodes are a special case that can be used
to improve the performance clusters with high queue,
exchange, or binding churn. RAM nodes do not provide
higher message rates. When in doubt, use
disk nodes only.</p>
<p>Since RAM nodes store internal database tables in RAM only, they must sync
them from a peer node on startup. This means that a cluster must contain
at least one disk node. It is therefore not possible to manually remove
the last remaining disk node in a cluster.</p>
<h2><a id="ram-nodes" class="anchor" href="#ram-nodes">Clusters with RAM nodes</a></h2>
<p>RAM nodes keep their metadata only in memory. As RAM nodes
don't have to write to disc as much as disc nodes, they can
perform better. However, note that since persistent queue
data is always stored on disc, the performance improvements
will affect only resource management (e.g. adding/removing
queues, exchanges, or vhosts), but not publishing or
consuming speed.</p>
<p>RAM nodes are an advanced use case; when setting up your
first cluster you should simply not use them. You should
have enough disc nodes to handle your redundancy
requirements, then if necessary add additional RAM nodes for
scale.</p>
<p>A cluster containing only RAM nodes would be too volatile; if the
cluster stops you will not be able to start it again and
<strong>will lose all data</strong>. RabbitMQ will prevent the creation of a
RAM-node-only cluster in many situations, but it can't
absolutely prevent it.</p>
<p>The examples here show a cluster with one disc and one RAM
node for simplicity only; such a cluster is a poor design
choice.</p>
<h3><a id="creating-ram" class="anchor" href="#creating-ram">Creating RAM nodes</a></h3>
<p>We can declare a node as a RAM node when it first joins
the cluster. We do this with
<span class="code ">rabbitmqctl join_cluster</span> as before, but passing the
<span class="code ">--ram</span> flag:</p>
<pre class="lang-bash">
# on rabbit2
rabbitmqctl stop_app
# =&gt; Stopping node rabbit@rabbit2 ...done.

rabbitmqctl join_cluster --ram rabbit@rabbit1
# =&gt; Clustering node rabbit@rabbit2 with [rabbit@rabbit1] ...done.

rabbitmqctl start_app
# =&gt; Starting node rabbit@rabbit2 ...done.
</pre>

<p>RAM nodes are shown as such in the cluster status:</p>
<pre class="lang-bash">
# on rabbit1
rabbitmqctl cluster_status
# =&gt; Cluster status of node rabbit@rabbit1 ...
# =&gt; [{nodes,[{disc,[rabbit@rabbit1]},{ram,[rabbit@rabbit2]}]},
# =&gt;  {running_nodes,[rabbit@rabbit2,rabbit@rabbit1]}]
# =&gt; ...done.

# on rabbit2
rabbitmqctl cluster_status
# =&gt; Cluster status of node rabbit@rabbit2 ...
# =&gt; [{nodes,[{disc,[rabbit@rabbit1]},{ram,[rabbit@rabbit2]}]},
# =&gt;  {running_nodes,[rabbit@rabbit1,rabbit@rabbit2]}]
# =&gt; ...done.
</pre>

<h3><a id="change-type" class="anchor" href="#change-type">Changing node types</a></h3>
<p>We can change the type of a node from ram to disc and vice
versa. Say we wanted to reverse the types of
<span class="code ">rabbit@rabbit2</span> and <span class="code ">rabbit@rabbit1</span>, turning
the former from a ram node into a disc node and the latter from a
disc node into a ram node. To do that we can use the
<span class="code ">change_cluster_node_type</span> command. The node must be
stopped first.</p>
<pre class="lang-bash">
# on rabbit2
rabbitmqctl stop_app
# =&gt; Stopping node rabbit@rabbit2 ...done.

rabbitmqctl change_cluster_node_type disc
# =&gt; Turning rabbit@rabbit2 into a disc node ...done.

rabbitmqctl start_app
# =&gt; Starting node rabbit@rabbit2 ...done.

# on rabbit1
rabbitmqctl stop_app
# =&gt; Stopping node rabbit@rabbit1 ...done.

rabbitmqctl change_cluster_node_type ram
# =&gt; Turning rabbit@rabbit1 into a ram node ...done.

rabbitmqctl start_app
# =&gt; Starting node rabbit@rabbit1 ...done.
</pre><div id="help-and-feedback"><h2>Getting Help and Providing Feedback</h2><p>
                    If you have questions about the contents of this guide or
                    any other topic related to RabbitMQ, don't hesitate to ask them
                    on the <a href="https://groups.google.com/forum/#!forum/rabbitmq-users">RabbitMQ mailing list</a>.
                  </p></div><div id="contribute"><h2>Help Us Improve the Docs &lt;3</h2><p>
                    If you'd like to contribute an improvement to the site,
                    its source is <a href="https://github.com/rabbitmq/rabbitmq-website">available on GitHub</a>.
                    Simply fork the repository and submit a pull request. Thank you!
                  </p></div></div><div id="right-nav"><div id="in-this-section"><h4>In This Section</h4><ul>
     <li><a href="/admin-guide.html" class="selected">Server Documentation</a><ul>
       <li><a href="/configure.html">Configuration</a></li>
       <li><a href="/management.html">Management UI</a></li>
       <li><a href="/monitoring.html">Monitoring</a></li>
       <li><a href="/production-checklist.html">Production Checklist</a></li>
       <li><a href="/ssl.html">TLS Support</a></li>
       <li><a href="/streams.html">Streams</a></li>
       <li><a href="/feature-flags.html">Feature Flags</a></li>
       <li><a href="/distributed.html">Distributed RabbitMQ</a></li>
       <li><a href="/clustering.html" class="selected">Clustering</a><ul>
         <li><a href="/cluster-formation.html">Cluster Formation</a></li>
         <li><a href="/ha.html">High Availability</a></li>
         <li><a href="/partitions.html">Network Partitions</a></li>
         <li><a href="/nettick.html">Net Tick Time</a></li>
         <li><a href="/clustering-ssl.html">TLS for Inter-node (Clustering) Traffic</a></li>
         <li><a href="/clustering-compression.html">Inter-node Traffic Compression</a></li>
       </ul></li>
       <li><a href="/reliability.html">Reliable Delivery</a></li>
       <li><a href="/definitions.html">Schema Definition Export and Import</a></li>
       <li><a href="/backup.html">Backup and restore</a></li>
       <li><a href="/alarms.html">Alarms</a></li>
       <li><a href="/memory-use.html">Memory Use</a></li>
       <li><a href="/networking.html">Networking</a></li>
       <li><a href="/vhosts.html">Virtual Hosts</a></li>
       <li><a href="/pacemaker.html">High Availability (pacemaker)</a></li>
       <li><a href="/access-control.html">Access Control (Authorisation)</a></li>
       <li><a href="/authentication.html">Authentication Mechanisms</a></li>
       <li><a href="/ldap.html">LDAP</a></li>
       <li><a href="/lazy-queues.html">Lazy Queues</a></li>
       <li><a href="/event-exchange.html">Internal Event Exchange</a></li>
       <li><a href="/firehose.html">Firehose (Message Tracing)</a></li>
       <li><a href="/manpages.html">Manual Pages</a></li>
       <li><a href="/windows-quirks.html">Windows Quirks</a></li>
       
       
       
       
     </ul></li>
     <li><a href="/clients.html">Client Documentation</a></li>
     <li><a href="/plugins.html">Plugins</a></li>
     <li><a href="/news.html">News</a></li>
     <li><a href="/protocol.html">Protocol</a></li>
     <li><a href="/extensions.html">Our Extensions</a></li>
     <li><a href="/build.html">Building</a></li>
     
     <li><a href="/mpl.html">License</a></li>
   </ul></div></div></div><div class="clear"></div><div class="pageFooter"><div class="container"></div><div class="container"><div class="rabbit-logo"><a href="/"><img src="/img/logo-rabbitmq-white.svg" alt="RabbitMQ" /></a></div><ul class="footerNav"><li><a href="/#features">Features</a></li><li><a href="/#getstarted">Get Started</a></li><li><a href="/#support">Support</a></li><li><a href="/#community">Community</a></li><li><a href="/documentation.html">Docs</a></li></ul><p id="copyright">
          Copyright Â© 2007-2021 <a href="https://tanzu.vmware.com/">VMware</a>, Inc. or its affiliates. All rights reserved.
          <a href="https://pivotal.io/legal">Terms of Use</a>,
          <a href="https://pivotal.io/privacy-policy">Privacy</a> and
          <a href="/trademark-guidelines.html">Trademark Guidelines</a><br /><a id="teconsent"></a></p></div></div></div><script type="text/javascript" src="/js/highlight.pack.js"></script><script type="text/javascript">
        // code highlighting
        window.addEventListener("load", function() {
          const selectors = "pre.lang-bash, \
                             pre.lang-csharp, \
                             pre.lang-elixir, \
                             pre.lang-erlang, \
                             pre.lang-go, \
                             pre.lang-groovy, \
                             pre.lang-haskell, \
                             pre.lang-html, \
                             pre.lang-ini, \
                             pre.lang-java, \
                             pre.lang-javascript, \
                             pre.lang-json, \
                             pre.lang-makefile, \
                             pre.lang-objectivec, \
                             pre.lang-php, \
                             pre.lang-plaintext, \
                             pre.lang-powershell, \
                             pre.lang-python, \
                             pre.lang-ruby, \
                             pre.lang-swift, \
                             pre.lang-yaml, \
                             pre.lang-xml";
          document.querySelectorAll(selectors).forEach(function(el) {
            hljs.highlightBlock(el);
          });
        });
      </script></body>
</html>
